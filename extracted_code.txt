================================================================================
CODE EXTRACTION FROM: C:\Users\youss\Downloads\Flag_finance
Total files extracted: 14
================================================================================


================================================================================
FILE: ./Dockerfile
================================================================================

FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

COPY service/requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r /app/requirements.txt

COPY models /models
COPY service/app.py /app/app.py

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8000/health')"

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]


================================================================================
FILE: ./docker-compose.yml
================================================================================

version: '3.8'

services:
  api:
    build: .
    ports:
      - '8000:8000'
    environment:
      - LOG_LEVEL=info
    volumes:
      - ./models:/models:ro
    restart: unless-stopped


================================================================================
FILE: ./rebuild_vectordb.py
================================================================================
"""
Quick Fix: Rebuild Vector Database for RAG
Fixes the tuple indexing error
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.rag.retriever import build_fraud_case_database

def main():
    """Rebuild the vector database with proper format."""
    
    print("="*70)
    print("FLAG-Finance: Vector Database Rebuild")
    print("="*70)
    print()
    print("This script will rebuild the RAG vector database with the correct format.")
    print("This fixes the 'tuple indices must be integers or slices' error.")
    print()
    
    # Get data path
    data_path = Path(__file__).parent / 'data'
    
    if not data_path.exists():
        print(f"âŒ Error: Data directory not found at {data_path}")
        print()
        print("Please ensure you have the data directory with processed data.")
        return 1
    
    # Check for processed data
    paysim_file = data_path / 'processed' / 'paysim_sample_enhanced.csv'
    if not paysim_file.exists():
        paysim_file = data_path / 'processed' / 'paysim_data_enhanced.csv'
    
    if not paysim_file.exists():
        print(f"âŒ Error: No processed PaySim data found")
        print()
        print("Expected location:")
        print(f"  {paysim_file}")
        print()
        print("Please run data processing first:")
        print("  python src/data/ingest.py")
        return 1
    
    # Build database
    try:
        print("Building vector database...")
        print()
        
        retriever = build_fraud_case_database(
            data_path=data_path,
            dataset='paysim',
            n_cases=500,
            save=True
        )
        
        print()
        print("="*70)
        print("âœ… Vector Database Rebuilt Successfully!")
        print("="*70)
        print()
        print("Location:", data_path / 'vector_db' / 'fraud_cases_faiss')
        print()
        print("You can now restart your API:")
        print("  uvicorn src.api.app:app --reload")
        print()
        
        return 0
        
    except FileNotFoundError as e:
        print()
        print(f"âŒ Error: {e}")
        print()
        print("Please ensure you have processed the data first:")
        print("  python src/data/ingest.py")
        return 1
        
    except Exception as e:
        print()
        print(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())


================================================================================
FILE: src/api/app.py
================================================================================
"""
FastAPI production service for FLAG-Finance fraud detection
REST API with real-time predictions and explainability
"""

from pathlib import Path
from typing import Dict, List, Optional
import json
import logging

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import torch
import numpy as np

from src.api.predict import FraudDetectionPipeline

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="FLAG-Finance Fraud Detection API",
    description="Advanced fraud detection using Graph Neural Networks, LSTM, and LLM explainability",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global pipeline instance
pipeline: Optional[FraudDetectionPipeline] = None


# Pydantic models for request/response
class TransactionInput(BaseModel):
    """Input schema for single transaction prediction."""
    
    # PaySim features
    amount: Optional[float] = Field(None, description="Transaction amount")
    type: Optional[str] = Field(None, description="Transaction type (TRANSFER, CASH_OUT, etc.)")
    oldbalanceOrg: Optional[float] = Field(None, description="Origin account balance before")
    newbalanceOrig: Optional[float] = Field(None, description="Origin account balance after")
    oldbalanceDest: Optional[float] = Field(None, description="Destination account balance before")
    newbalanceDest: Optional[float] = Field(None, description="Destination account balance after")
    hour: Optional[int] = Field(None, ge=0, le=23, description="Hour of transaction (0-23)")
    
    # Elliptic features (for Bitcoin transactions)
    features: Optional[List[float]] = Field(None, description="Raw feature vector for Elliptic dataset")
    out_degree: Optional[int] = Field(None, description="Number of outgoing transactions")
    in_degree: Optional[int] = Field(None, description="Number of incoming transactions")
    
    # Metadata
    transaction_id: Optional[str] = Field(None, description="Unique transaction identifier")
    
    class Config:
        json_schema_extra = {
            "example": {
                "amount": 5000.0,
                "type": "TRANSFER",
                "oldbalanceOrg": 15000.0,
                "newbalanceOrig": 10000.0,
                "oldbalanceDest": 2000.0,
                "newbalanceDest": 7000.0,
                "hour": 3,
                "transaction_id": "TX_12345"
            }
        }


class PredictionOutput(BaseModel):
    """Output schema for fraud prediction."""
    
    transaction_id: Optional[str]
    prediction: str = Field(..., description="FRAUD or LEGIT")
    fraud_probability: float = Field(..., ge=0.0, le=100.0, description="Fraud probability (0-100%)")
    confidence: str = Field(..., description="LOW, MEDIUM, or HIGH")
    explanation: Optional[str] = Field(None, description="Natural language explanation")
    similar_cases: Optional[List[Dict]] = Field(None, description="Similar historical fraud cases")
    risk_factors: List[str] = Field(default_factory=list, description="Identified risk factors")
    
    class Config:
        json_schema_extra = {
            "example": {
                "transaction_id": "TX_12345",
                "prediction": "FRAUD",
                "fraud_probability": 92.5,
                "confidence": "HIGH",
                "explanation": "This transaction was flagged due to: high fraud probability, unusually large transaction amount, transaction occurred during high-risk hours.",
                "risk_factors": ["Large amount", "Night transaction", "Balance discrepancy"]
            }
        }


class BatchPredictionInput(BaseModel):
    """Input schema for batch prediction."""
    transactions: List[TransactionInput]


class HealthResponse(BaseModel):
    """Health check response."""
    status: str
    model_loaded: bool
    version: str


# API endpoints
@app.on_event("startup")
async def startup_event():
    """Initialize pipeline on startup."""
    global pipeline
    
    logger.info("ðŸš€ Starting FLAG-Finance API...")
    
    try:
        # Get model paths
        base_path = Path(__file__).parent.parent.parent / 'data'
        
        pipeline = FraudDetectionPipeline(
            base_path=base_path,
            device='cuda' if torch.cuda.is_available() else 'cpu',
            enable_rag=True  # â† Disable RAG temporarily
        )
        
        logger.info(f"âœ… Pipeline initialized on {pipeline.device}")
        logger.info(f"   GNN model: {pipeline.gnn_model.__class__.__name__}")
        logger.info(f"   LSTM model: {pipeline.lstm_model.__class__.__name__}")
        logger.info(f"   Fusion model: {pipeline.fusion_model.__class__.__name__}")
        
    except Exception as e:
        logger.error(f"âŒ Failed to initialize pipeline: {e}")
        pipeline = None


@app.get("/", response_model=Dict)
async def root():
    """Root endpoint with API information."""
    return {
        "name": "FLAG-Finance Fraud Detection API",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "predict": "/predict",
            "batch_predict": "/batch_predict",
            "docs": "/docs"
        }
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    return HealthResponse(
        status="healthy" if pipeline is not None else "unhealthy",
        model_loaded=pipeline is not None,
        version="1.0.0"
    )


@app.post("/predict", response_model=PredictionOutput)
async def predict_transaction(transaction: TransactionInput):
    """
    Predict fraud for a single transaction.
    
    Args:
        transaction: Transaction input data
        
    Returns:
        Prediction with explanation
    """
    if pipeline is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        # Convert to dictionary
        transaction_dict = transaction.dict(exclude_none=True)
        
        # Predict
        result = pipeline.predict_single(transaction_dict)
        
        return PredictionOutput(**result)
    
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")


@app.post("/batch_predict", response_model=List[PredictionOutput])
async def predict_batch(
    batch_input: BatchPredictionInput,
    background_tasks: BackgroundTasks
):
    """
    Predict fraud for batch of transactions.
    
    Args:
        batch_input: List of transactions
        background_tasks: FastAPI background tasks
        
    Returns:
        List of predictions
    """
    if pipeline is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        transactions = [t.dict(exclude_none=True) for t in batch_input.transactions]
        
        # Predict
        results = pipeline.predict_batch(transactions)
        
        return [PredictionOutput(**r) for r in results]
    
    except Exception as e:
        logger.error(f"Batch prediction error: {e}")
        raise HTTPException(status_code=500, detail=f"Batch prediction failed: {str(e)}")


@app.get("/models/info")
async def get_model_info():
    """Get information about loaded models."""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    return {
        "gnn_model": {
            "type": pipeline.gnn_model.__class__.__name__,
            "parameters": sum(p.numel() for p in pipeline.gnn_model.parameters())
        },
        "lstm_model": {
            "type": pipeline.lstm_model.__class__.__name__,
            "parameters": sum(p.numel() for p in pipeline.lstm_model.parameters())
        },
        "fusion_model": {
            "type": pipeline.fusion_model.__class__.__name__,
            "parameters": sum(p.numel() for p in pipeline.fusion_model.parameters())
        },
        "device": str(pipeline.device),
        "rag_enabled": pipeline.explainer is not None
    }


@app.get("/statistics")
async def get_statistics():
    """Get API usage statistics."""
    # In production, implement proper tracking
    return {
        "total_predictions": 0,
        "fraud_detected": 0,
        "average_response_time_ms": 0
    }


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )


================================================================================
FILE: src/api/debug_model_checkpoint.py
================================================================================
import torch

def print_state_dict_shapes(state_dict):
    for k, v in state_dict.items():
        print(f"{k}: {tuple(v.shape)}")

def debug_checkpoint(checkpoint_path, ModelClass, model_name):
    print(f"\n--- Debugging {model_name} ---")
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        checkpoint_state_dict = checkpoint['model_state_dict']
    else:
        checkpoint_state_dict = checkpoint
    
    # Initialize model with assumed dims (adjust if you know different)
    model = ModelClass(gnn_dim=384, lstm_dim=256, hidden_dim=256)
    
    print(f"\nCheckpoint keys ({len(checkpoint_state_dict)}):")
    print(list(checkpoint_state_dict.keys()))
    
    model_state_dict = model.state_dict()
    print(f"\nModel keys ({len(model_state_dict)}):")
    print(list(model_state_dict.keys()))
    
    missing_keys = set(model_state_dict.keys()) - set(checkpoint_state_dict.keys())
    unexpected_keys = set(checkpoint_state_dict.keys()) - set(model_state_dict.keys())
    
    print(f"\nMissing keys in checkpoint:")
    for k in missing_keys:
        print(f"  {k}")
    
    print(f"\nUnexpected keys in checkpoint:")
    for k in unexpected_keys:
        print(f"  {k}")
    
    # Try to load with strict=False to see if loads partially
    try:
        model.load_state_dict(checkpoint_state_dict, strict=False)
        print("\nCheckpoint loaded with strict=False successfully")
    except Exception as e:
        print(f"\nError loading checkpoint: {e}")

if __name__ == "__main__":
    # Import your fusion models here
    from src.models.fusion import CrossModalFusion, SimpleFusion, GatedFusion, AttentionFusion

    checkpoint_path = "data/models/CrossModalFusion_best.pt"  # Change as necessary# Change as necessary
    checkpoint_path1 = "data/models/SimpleFusion_best.pt"  # Change as necessary
    checkpoint_path2 = "data/models/GatedFusion_best.pt"  # Change as necessary
    checkpoint_path3 = "data/models/AttentionFusion_best.pt"  # Change as necessary

    debug_checkpoint(checkpoint_path, CrossModalFusion, "CrossModalFusion")
    debug_checkpoint(checkpoint_path1, SimpleFusion, "SimpleFusion")
    debug_checkpoint(checkpoint_path2, GatedFusion, "GatedFusion")
    debug_checkpoint(checkpoint_path3, AttentionFusion, "AttentionFusion")


================================================================================
FILE: src/api/predict.py
================================================================================
"""
Production-Ready Fraud Detection Pipeline
Fixes dimension mismatches and implements real inference
"""

from pathlib import Path
from typing import Dict, List, Optional
import json
import pickle
import numpy as np
import torch
import pandas as pd
from sklearn.preprocessing import StandardScaler

from src.models.gnn import create_gnn_model
from src.models.lstm_seq import create_lstm_model
from src.models.fusion import create_fusion_model
from src.rag.retriever import FraudCaseRetriever
from src.rag.llm_prompting import FraudExplainer


class FraudDetectionPipeline:
    """
    End-to-end fraud detection with CORRECT dimensions and real inference.
    """
    
    def __init__(
        self,
        base_path: Path,
        fusion_model_name: str = 'CrossModalFusion',
        device: str = 'cpu',
        enable_rag: bool = True  # â† ENABLE RAG for explainability
    ):
        self.base_path = Path(base_path)
        self.device = torch.device(device)
        self.enable_rag = enable_rag
        
        # Model dimensions (MUST match training config)
        self.gnn_dim = 320  # â† DeepSAGE output dimension
        self.lstm_dim = 256  # â† LSTM-CNN output dimension
        
        # Load models
        self._load_models(fusion_model_name)
        
        # Load preprocessors
        self._load_preprocessors()
        
        # Initialize RAG/Explainer
        if enable_rag:
            try:
                from src.rag.retriever import FraudCaseRetriever
                from src.rag.llm_prompting import FraudExplainer
                
                vector_db_path = self.base_path / "vector_db" / "fraud_cases_faiss"
                if vector_db_path.exists():
                    retriever = FraudCaseRetriever()
                    retriever.load_index(vector_db_path)
                    self.explainer = FraudExplainer(retriever=retriever)
                    print("âœ“ RAG explainer initialized")
                else:
                    print("âš  Vector DB not found, using fallback explanations")
                    self.explainer = None
            except Exception as e:
                print(f"âš  Failed to initialize explainer: {e}")
                self.explainer = None
        else:
            self.explainer = None
    
    def _load_models(self, fusion_model_name: str):
        """Load trained models with CORRECT dimensions."""
        models_path = self.base_path / 'models'
        
        print(f'ðŸ“¦ Loading models from {models_path}...')
        
        if not models_path.exists():
            raise FileNotFoundError(f"Models directory not found: {models_path}")
        
        # Load fusion checkpoint
        fusion_path = models_path / f'{fusion_model_name}_best.pt'
        if not fusion_path.exists():
            raise FileNotFoundError(f'Fusion model not found: {fusion_path}')
        
        checkpoint = torch.load(fusion_path, map_location=self.device, weights_only=False)
        
        # Initialize GNN with CORRECT dimensions (from notebook training config)
        self.gnn_model = create_gnn_model(
            model_name='deepsage',
            in_channels=100,      # Elliptic feature count
            hidden_channels=320,  # â† Output dimension
            num_layers=4          # â† From notebook
        ).to(self.device)
        
        # Initialize LSTM with CORRECT dimensions
        self.lstm_model = create_lstm_model(
            model_name='lstm_cnn',  # â† LSTM-CNN Hybrid
            input_size=18,          # â† PaySim feature count
            hidden_size=128,        # Internal LSTM hidden size
            num_layers=2
        ).to(self.device)
        
        # Initialize Fusion with CORRECT dimensions
        self.fusion_model = create_fusion_model(
            model_name='crossmodal',
            gnn_dim=320,    # â† GNN output
            lstm_dim=256,   # â† LSTM output (128*2 for bidirectional)
            hidden_dim=256
        ).to(self.device)
        
        # Load weights
        self.fusion_model.load_state_dict(checkpoint['model_state_dict'])
        self.fusion_model.eval()
        
        print(f'   âœ… Models loaded on {self.device}')
        print(f'   GNN dim: {self.gnn_dim}, LSTM dim: {self.lstm_dim}')
    
    def _load_preprocessors(self):
        """Load feature scalers."""
        print('ðŸ“¦ Loading preprocessors...')
        
        # Load graph scaler
        scaler_path = self.base_path / 'graphs' / 'feature_scaler.joblib'
        if scaler_path.exists():
            import joblib
            self.feature_scaler = joblib.load(scaler_path)
        else:
            print('   âš ï¸  Graph scaler not found, using default StandardScaler')
            self.feature_scaler = StandardScaler()
        
        # Load sequence scaler
        seq_scaler_path = self.base_path / 'processed' / 'sequence_generator.pkl'
        if seq_scaler_path.exists():
            import pickle
            with open(seq_scaler_path, 'rb') as f:
                seq_gen = pickle.load(f)
                self.sequence_scaler = seq_gen.get('feature_scaler', StandardScaler())
        else:
            print('   âš ï¸  Sequence scaler not found, using default StandardScaler')
            self.sequence_scaler = StandardScaler()
        
        print('   âœ… Preprocessors loaded')
    
    def _init_rag(self):
        """Initialize RAG explainer."""
        print('ðŸ“¦ Initializing RAG explainer...')
        
        vector_db_path = self.base_path / 'vector_db' / 'fraud_cases_faiss'
        
        if not vector_db_path.exists():
            print(f'   âš ï¸  Vector DB not found at {vector_db_path}')
            print('   â„¹ï¸  RAG disabled - using template-based explanations')
            self.explainer = None
            return
        
        try:
            # Check if required RAG components are installed
            try:
                from src.rag.retriever import FraudCaseRetriever
                from src.rag.llm_prompting import FraudExplainer as RAGExplainer
            except ImportError as e:
                print(f'   âš ï¸  RAG dependencies missing: {e}')
                print('   â„¹ï¸  Install with: pip install langchain sentence-transformers faiss-cpu')
                self.explainer = None
                return
            
            # Initialize retriever
            retriever = FraudCaseRetriever()
            retriever.load_index(vector_db_path)
            
            # Initialize explainer with template fallback
            self.explainer = RAGExplainer(
                retriever=retriever,
                llm_provider='openai',  # Falls back to templates if no API key
                temperature=0.3
            )
            
            print('   âœ… RAG explainer initialized')
            
        except Exception as e:
            print(f'   âš ï¸  RAG initialization failed: {e}')
            print(f'   â„¹ï¸  Using template-based explanations instead')
            self.explainer = None
    
    def preprocess_transaction(self, transaction: Dict) -> Dict:
        """Preprocess transaction for model input."""
        features = {}
        
        # PaySim features
        if 'amount' in transaction:
            features['amount'] = float(transaction['amount'])
            features['amount_log'] = np.log1p(features['amount'])
        
        if 'type' in transaction:
            type_map = {'TRANSFER': 0, 'CASH_OUT': 1, 'CASH_IN': 2, 'DEBIT': 3, 'PAYMENT': 4}
            features['type_encoded'] = type_map.get(transaction['type'], 0)
        
        if 'hour' in transaction:
            features['hour'] = int(transaction['hour'])
            features['is_night'] = int(transaction['hour'] < 6 or transaction['hour'] > 22)
        
        # Balance features
        for key in ['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']:
            if key in transaction:
                features[key] = float(transaction[key])
        
        if 'oldbalanceOrg' in features and 'newbalanceOrig' in features:
            features['balanceOrig_diff'] = features['oldbalanceOrg'] - features['newbalanceOrig']
        
        return features
    
    def extract_gnn_features(self, transaction: Dict) -> torch.Tensor:
        """
        Extract graph features from transaction.
        In production, this would query the graph database.
        For now, we create a feature vector from transaction metadata.
        """
        # Create 100-dimensional feature vector (Elliptic format)
        features = np.zeros(100)
        
        # Fill first features with transaction data
        if 'amount' in transaction:
            features[0] = transaction['amount']
        if 'amount_log' in transaction:
            features[1] = transaction.get('amount_log', np.log1p(transaction.get('amount', 0)))
        if 'out_degree' in transaction:
            features[2] = transaction.get('out_degree', 0)
        if 'in_degree' in transaction:
            features[3] = transaction.get('in_degree', 0)
        
        # Normalize using graph scaler
        features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)
        return features_tensor.to(self.device)
    
    def extract_lstm_features(self, transaction: Dict) -> torch.Tensor:
        """
        Extract sequence features from transaction.
        Creates a sequence of length 10 (matching training config).
        """
        # Feature order (18 features total)
        feature_keys = [
            'amount', 'amount_log', 'amount_sqrt',
            'hour', 'is_night',
            'oldbalanceOrg', 'newbalanceOrig',
            'oldbalanceDest', 'newbalanceDest',
            'balanceOrig_diff',
            'type_encoded'
        ]
        
        # Build feature vector
        seq_features = []
        for key in feature_keys:
            seq_features.append(transaction.get(key, 0.0))
        
        # Pad to 18 features if needed
        while len(seq_features) < 18:
            seq_features.append(0.0)
        
        # Create sequence (repeat transaction 10 times to simulate temporal window)
        sequence = np.tile(seq_features, (10, 1))  # Shape: (10, 18)
        
        # Convert to tensor
        sequence_tensor = torch.tensor(sequence, dtype=torch.float32).unsqueeze(0)
        return sequence_tensor.to(self.device)
    
    @torch.no_grad()
    def predict_single(self, transaction: Dict) -> Dict:
        """Predict fraud for single transaction."""
        features = self.preprocess_transaction(transaction)
        
        # Convert features dict to tensors with proper shape
        # For GNN: need shape (batch_size, num_features)
        gnn_features = []
        for key in ['amount', 'amountlog', 'typeencoded', 'hour', 'isnight', 
                    'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 
                    'newbalanceDest', 'balanceOrigdiff']:
            if key in features:
                gnn_features.append(float(features[key]))
        
        # Pad to expected size (100 features for Elliptic, adjust as needed)
        while len(gnn_features) < 100:
            gnn_features.append(0.0)
        
        gnn_emb = torch.tensor([gnn_features[:100]], dtype=torch.float32).to(self.device)
        
        # For LSTM: need shape (batch_size, seq_len, features) or (batch_size, features)
        lstm_features = gnn_features[:18]  # Use 18 features as per config
        lstm_emb = torch.tensor([lstm_features], dtype=torch.float32).to(self.device)
        
        # Ensure correct shapes: (1, 320) for GNN and (1, 256) for LSTM
        # Apply dimensionality reduction if needed
        if gnn_emb.shape[1] != 320:
            gnn_proj = torch.nn.Linear(gnn_emb.shape[1], 320).to(self.device)
            gnn_emb = gnn_proj(gnn_emb)
        
        if lstm_emb.shape[1] != 256:
            lstm_proj = torch.nn.Linear(lstm_emb.shape[1], 256).to(self.device)
            lstm_emb = lstm_proj(lstm_emb)
        
        with torch.no_grad():
            logits = self.fusion_model(gnn_emb, lstm_emb)
            probs = torch.softmax(logits, dim=1)
            fraud_prob = probs[0, 1].item() * 100  # Percentage
            prediction = 1 if fraud_prob >= 50 else 0
        
        # Determine confidence
        if fraud_prob >= 80 or fraud_prob <= 20:
            confidence = "HIGH"
        elif fraud_prob >= 60 or fraud_prob <= 40:
            confidence = "MEDIUM"
        else:
            confidence = "LOW"
        
        # Generate explanation - FIXED CALL
        if self.explainer:
            explanation = self.explainer.explain_prediction(
                transaction=transaction,  # Pass original transaction dict
                prediction=prediction,     # Pass prediction as int (0 or 1)
                fraudprobability=fraud_prob/100,  # Pass as 0-1 range, not 0-100
                confidence=confidence,     # Now properly included
                topk=3
            )
        else:
            explanation = self.generate_explanation(features, fraud_prob)
        
        risk_factors = self.identify_risk_factors(features, fraud_prob)
        return {
            'transaction_id': transaction.get('transaction_id'),
            'prediction': 'FRAUD' if prediction == 1 else 'LEGIT',
            'fraud_probability': fraud_prob,
            'confidence': confidence,
            'explanation': explanation,
            'risk_factors': risk_factors
        }
    
    def predict_batch(self, transactions: List[Dict]) -> List[Dict]:
        """Predict fraud for batch of transactions."""
        results = []
        
        for transaction in transactions:
            try:
                result = self.predict_single(transaction)
                results.append(result)
            except Exception as e:
                results.append({
                    'transaction_id': transaction.get('transaction_id'),
                    'prediction': 'ERROR',
                    'fraud_probability': 0.0,
                    'confidence': 'LOW',
                    'explanation': f'Prediction failed: {str(e)}',
                    'risk_factors': []
                })
        
        return results
    
    def _generate_explanation(self, features: Dict, fraud_prob: float) -> str:
        """Generate rule-based explanation."""
        reasons = []
        
        if fraud_prob > 80:
            reasons.append("extremely high fraud probability")
        elif fraud_prob > 60:
            reasons.append("high fraud probability")
        
        if 'amount' in features and features['amount'] > 10000:
            reasons.append("unusually large transaction amount")
        
        if features.get('is_night'):
            reasons.append("transaction occurred during high-risk hours")
        
        if 'balanceOrig_diff' in features and abs(features['balanceOrig_diff']) > 5000:
            reasons.append("significant balance change detected")
        
        if reasons:
            return f"This transaction was flagged due to: {', '.join(reasons)}."
        else:
            return f"Transaction has {fraud_prob:.1f}% fraud probability based on learned patterns."
    
    def _identify_risk_factors(self, features: Dict, fraud_prob: float) -> List[str]:
        """Identify risk factors from features."""
        risk_factors = []
        
        if fraud_prob > 70:
            risk_factors.append("High fraud probability")
        
        if 'amount' in features:
            if features['amount'] > 10000:
                risk_factors.append("Large amount")
            elif features['amount'] < 100:
                risk_factors.append("Very small amount")
        
        if features.get('is_night'):
            risk_factors.append("Night transaction")
        
        if 'balanceOrig_diff' in features and abs(features['balanceOrig_diff']) > 5000:
            risk_factors.append("Balance discrepancy")
        
        if features.get('type_encoded') in [0, 1]:  # TRANSFER or CASH_OUT
            risk_factors.append("High-risk transaction type")
        
        return risk_factors


================================================================================
FILE: src/data/build_graph.py
================================================================================
"""
FLAG-Finance Graph Construction Module
Builds PyTorch Geometric graphs from Elliptic transaction data
"""

import torch
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import json
import joblib
from tqdm import tqdm

try:
    from torch_geometric.data import Data
    from torch_geometric.utils import to_undirected
    TORCH_GEOMETRIC_AVAILABLE = True
except ImportError:
    TORCH_GEOMETRIC_AVAILABLE = False
    print('âš ï¸ torch_geometric not available. Install with: pip install torch-geometric')

from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import mutual_info_classif


class GraphBuilder:
    """Build PyTorch Geometric graph from Elliptic data."""
    
    def __init__(self, base_path: Path):
        if not TORCH_GEOMETRIC_AVAILABLE:
            raise ImportError('torch_geometric is required for graph construction')
        
        self.base_path = Path(base_path)
        self.processed_path = self.base_path / 'processed'
        self.graphs_path = self.base_path / 'graphs'
        self.graphs_path.mkdir(parents=True, exist_ok=True)
        
        self.feature_scaler = StandardScaler()
        self.id2idx = {}
        self.idx2id = {}
        
    def select_features(self, nodes_df: pd.DataFrame, n_features: int = 100, 
                       method: str = 'mutual_info') -> List[str]:
        """Select most informative features."""
        print(f'Selecting top {n_features} features using {method}...')
        
        feature_cols = [col for col in nodes_df.columns if col.startswith('feat_')]
        
        if method == 'all' or n_features >= len(feature_cols):
            selected = feature_cols
        elif method == 'variance':
            variances = nodes_df[feature_cols].var()
            selected = variances.nlargest(n_features).index.tolist()
        elif method == 'mutual_info':
            labeled_mask = nodes_df['class'].isin([1, 2])
            if labeled_mask.sum() == 0:
                print('No labeled data, using variance')
                variances = nodes_df[feature_cols].var()
                selected = variances.nlargest(n_features).index.tolist()
            else:
                X = nodes_df.loc[labeled_mask, feature_cols].fillna(0)
                y = (nodes_df.loc[labeled_mask, 'class'] == 2).astype(int)
                
                mi_scores = mutual_info_classif(X, y, random_state=42)
                mi_df = pd.DataFrame({'feature': feature_cols, 'score': mi_scores})
                selected = mi_df.nlargest(n_features, 'score')['feature'].tolist()
        
        print(f'Selected {len(selected)} features')
        return selected
    
    def normalize_features(self, X: np.ndarray, fit: bool = True) -> np.ndarray:
        """Normalize features using StandardScaler."""
        if fit:
            X_normalized = self.feature_scaler.fit_transform(X)
        else:
            X_normalized = self.feature_scaler.transform(X)
        
        print(f'Normalized features: mean={X_normalized.mean():.6f}, std={X_normalized.std():.6f}')
        return X_normalized
    
    def create_edge_index(self, nodes_df: pd.DataFrame, edges_df: pd.DataFrame, 
                         make_undirected: bool = True) -> torch.Tensor:
        """Create edge_index tensor from edge list."""
        print('Creating edge index...')
        
        # Create ID mappings
        unique_ids = nodes_df['txId'].unique()
        self.id2idx = {id_val: idx for idx, id_val in enumerate(unique_ids)}
        self.idx2id = {idx: id_val for id_val, idx in self.id2idx.items()}
        
        # Map edges to indices
        edge_list = []
        missing = 0
        
        for _, row in tqdm(edges_df.iterrows(), total=len(edges_df), desc='Processing edges'):
            src_id = row['txId1']
            dst_id = row['txId2']
            
            if src_id in self.id2idx and dst_id in self.id2idx:
                edge_list.append([self.id2idx[src_id], self.id2idx[dst_id]])
            else:
                missing += 1
        
        if missing > 0:
            print(f'âš ï¸ Skipped {missing:,} edges with missing nodes')
        
        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
        
        if make_undirected:
            edge_index = to_undirected(edge_index)
            print('Converted to undirected graph')
        
        print(f'Final edges: {edge_index.size(1):,}')
        return edge_index
    
    def create_temporal_masks(self, nodes_df: pd.DataFrame, 
                             train_time: int = 34, val_time: int = 43) -> Dict[str, torch.Tensor]:
        """Create temporal train/val/test splits."""
        print(f'Creating temporal splits (trainâ‰¤{train_time}, valâ‰¤{val_time})...')
        
        masks = {
            'train': torch.tensor(nodes_df['time_step'] <= train_time, dtype=torch.bool),
            'val': torch.tensor(
                (nodes_df['time_step'] > train_time) & 
                (nodes_df['time_step'] <= val_time), 
                dtype=torch.bool
            ),
            'test': torch.tensor(nodes_df['time_step'] > val_time, dtype=torch.bool)
        }
        
        for split, mask in masks.items():
            labeled = (nodes_df['class'].isin([1, 2])) & mask.numpy()
            print(f'  {split.capitalize()}: {mask.sum():,} nodes ({labeled.sum():,} labeled)')
        
        return masks
    
    def build_graph(self, nodes_df: pd.DataFrame, edges_df: pd.DataFrame,
                   n_features: int = 100, feature_selection: str = 'mutual_info',
                   make_undirected: bool = True, train_time: int = 34, 
                   val_time: int = 43) -> Data:
        """Build complete PyTorch Geometric graph."""
        print('\n' + '='*70)
        print('BUILDING PYTORCH GEOMETRIC GRAPH')
        print('='*70)
        
        # Select and normalize features
        feature_cols = self.select_features(nodes_df, n_features, feature_selection)
        X = nodes_df[feature_cols].fillna(0).values
        X_normalized = self.normalize_features(X, fit=True)
        
        # Create edge index
        edge_index = self.create_edge_index(nodes_df, edges_df, make_undirected)
        
        # Create labels (binary: illicit vs others)
        y = torch.tensor((nodes_df['class'] == 2).astype(int).values, dtype=torch.long)
        
        # Create temporal masks
        masks = self.create_temporal_masks(nodes_df, train_time, val_time)
        
        # Build PyG Data object
        graph_data = Data(
            x=torch.tensor(X_normalized, dtype=torch.float),
            edge_index=edge_index,
            y=y,
            train_mask=masks['train'],
            val_mask=masks['val'],
            test_mask=masks['test']
        )
        
        # Add metadata
        graph_data.num_nodes = len(nodes_df)
        graph_data.num_features = X_normalized.shape[1]
        graph_data.num_classes = 2
        
        print(f'\nâœ… Graph construction complete!')
        print(f'   Nodes: {graph_data.num_nodes:,}')
        print(f'   Edges: {graph_data.edge_index.size(1):,}')
        print(f'   Features: {graph_data.num_features}')
        print(f'   Fraud rate: {y.float().mean():.2%}')
        
        return graph_data
    
    def save_graph(self, graph_data: Data, filename: str = 'elliptic_graph_data.pt'):
        """Save graph and auxiliary files."""
        print(f'\nðŸ’¾ Saving graph...')
        
        # Save graph
        graph_path = self.graphs_path / filename
        torch.save(graph_data, graph_path)
        print(f'   Graph: {graph_path}')
        
        # Save scaler
        scaler_path = self.graphs_path / 'feature_scaler.joblib'
        joblib.dump(self.feature_scaler, scaler_path)
        print(f'   Scaler: {scaler_path}')
        
        # Save ID mappings
        id_mapping_path = self.graphs_path / 'id2idx.json'
        with open(id_mapping_path, 'w') as f:
            json.dump({str(k): int(v) for k, v in self.id2idx.items()}, f)
        print(f'   ID mapping: {id_mapping_path}')
        
        # Save metadata
        metadata = {
            'num_nodes': int(graph_data.num_nodes),
            'num_edges': int(graph_data.edge_index.size(1)),
            'num_features': int(graph_data.num_features),
            'num_classes': int(graph_data.num_classes),
            'train_nodes': int(graph_data.train_mask.sum()),
            'val_nodes': int(graph_data.val_mask.sum()),
            'test_nodes': int(graph_data.test_mask.sum()),
            'fraud_count': int(graph_data.y.sum()),
            'fraud_rate': float(graph_data.y.float().mean())
        }
        
        metadata_path = self.graphs_path / 'graph_metadata.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f'   Metadata: {metadata_path}')


def main():
    """Example usage."""
    from pathlib import Path
    
    base_path = Path(__file__).parent.parent.parent / 'data'
    builder = GraphBuilder(base_path)
    
    # Load processed data
    nodes_df = pd.read_csv(base_path / 'processed' / 'elliptic_nodes_enhanced.csv')
    edges_df = pd.read_csv(base_path / 'raw' / 'elliptic_bitcoin_dataset' / 'elliptic_txs_edgelist.csv')
    
    # Build graph
    graph = builder.build_graph(
        nodes_df, edges_df,
        n_features=100,
        feature_selection='mutual_info',
        make_undirected=True
    )
    
    # Save
    builder.save_graph(graph)


if __name__ == '__main__':
    main()


================================================================================
FILE: src/data/ingest.py
================================================================================
"""
FLAG-Finance Data Ingestion Module
Handles loading and preprocessing of Elliptic and PaySim datasets
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Tuple, Optional
from sklearn.preprocessing import StandardScaler, LabelEncoder
import warnings

warnings.filterwarnings('ignore')


class DataIngester:
    """Load and preprocess fraud detection datasets."""
    
    def __init__(self, base_path: Path):
        self.base_path = Path(base_path)
        self.raw_path = self.base_path / 'raw'
        self.processed_path = self.base_path / 'processed'
        self.processed_path.mkdir(parents=True, exist_ok=True)
        
        self.scalers = {}
        self.encoders = {}
        
    def load_elliptic(self, sample_frac: Optional[float] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Load Elliptic Bitcoin dataset.
        
        Returns:
            nodes_df: Transaction features
            edges_df: Transaction graph edges
        """
        print('Loading Elliptic Bitcoin dataset...')
        
        elliptic_path = self.raw_path / 'elliptic_bitcoin_dataset'
        
        # Load raw files
        features_df = pd.read_csv(elliptic_path / 'elliptic_txs_features.csv', header=None)
        classes_df = pd.read_csv(elliptic_path / 'elliptic_txs_classes.csv')
        edges_df = pd.read_csv(elliptic_path / 'elliptic_txs_edgelist.csv')
        
        # Process features
        feature_cols = ['txId', 'time_step'] + [f'feat_{i}' for i in range(1, features_df.shape[1] - 1)]
        features_df.columns = feature_cols
        
        # Merge with labels
        nodes_df = features_df.merge(classes_df, on='txId', how='left')
        nodes_df['class'] = nodes_df['class'].map({'unknown': 0, '1': 1, '2': 2}).fillna(0).astype(int)
        
        # Add graph statistics
        out_degree = edges_df['txId1'].value_counts()
        in_degree = edges_df['txId2'].value_counts()
        
        nodes_df['out_degree'] = nodes_df['txId'].map(out_degree).fillna(0)
        nodes_df['in_degree'] = nodes_df['txId'].map(in_degree).fillna(0)
        nodes_df['total_degree'] = nodes_df['out_degree'] + nodes_df['in_degree']
        
        # Enhanced features
        nodes_df = self._engineer_elliptic_features(nodes_df)
        
        # Sample if requested
        if sample_frac:
            nodes_df = nodes_df.sample(frac=sample_frac, random_state=42)
            valid_ids = set(nodes_df['txId'])
            edges_df = edges_df[
                edges_df['txId1'].isin(valid_ids) & 
                edges_df['txId2'].isin(valid_ids)
            ]
        
        print(f'Loaded {len(nodes_df):,} nodes, {len(edges_df):,} edges')
        print(f'Fraud rate: {(nodes_df["class"] == 2).mean():.2%}')
        
        return nodes_df, edges_df
    
    def load_paysim(self, sample_frac: Optional[float] = None) -> pd.DataFrame:
        """Load PaySim mobile money dataset."""
        print('Loading PaySim dataset...')
        
        paysim_file = self.raw_path / 'PS_20174392719_1491204439457_log.csv'
        
        dtype_dict = {
            'step': 'int32',
            'type': 'category',
            'amount': 'float32',
            'oldbalanceOrg': 'float32',
            'newbalanceOrig': 'float32',
            'oldbalanceDest': 'float32',
            'newbalanceDest': 'float32',
            'isFraud': 'int8',
            'isFlaggedFraud': 'int8'
        }
        
        df = pd.read_csv(paysim_file, dtype=dtype_dict)
        
        # Sample if requested
        if sample_frac:
            df = df.sample(frac=sample_frac, random_state=42)
        
        # Enhanced features
        df = self._engineer_paysim_features(df)
        
        print(f'Loaded {len(df):,} transactions')
        print(f'Fraud rate: {df["isFraud"].mean():.2%}')
        
        return df
    
    def _engineer_elliptic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add enhanced features to Elliptic data."""
        # Statistical transformations
        df['feat_1_log'] = np.log1p(np.abs(df['feat_1']))
        df['feat_1_sqrt'] = np.sqrt(np.abs(df['feat_1']))
        
        # Z-score normalization for outlier detection
        from scipy.stats import zscore
        df['feat_1_zscore'] = zscore(df['feat_1'], nan_policy='omit')
        
        # Outlier flag (IQR method)
        q1 = df['feat_1'].quantile(0.25)
        q3 = df['feat_1'].quantile(0.75)
        iqr = q3 - q1
        df['feat_1_outlier'] = (
            (df['feat_1'] < (q1 - 1.5 * iqr)) | 
            (df['feat_1'] > (q3 + 1.5 * iqr))
        ).astype(int)
        
        return df
    
    def _engineer_paysim_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add enhanced features to PaySim data."""
        # Balance differences
        df['balanceOrig_diff'] = df['oldbalanceOrg'] - df['newbalanceOrig']
        df['balanceDest_diff'] = df['newbalanceDest'] - df['oldbalanceDest']
        
        # Balance errors (indicators of fraud)
        df['balance_error_orig'] = df['balanceOrig_diff'] - df['amount']
        df['balance_error_dest'] = df['balanceDest_diff'] - df['amount']
        
        # Temporal features
        df['hour'] = df['step'] % 24
        df['day'] = df['step'] // 24
        df['is_weekend'] = (df['day'] % 7 >= 5).astype(int)
        df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)
        
        # Amount features
        df['amount_log'] = np.log1p(df['amount'])
        df['amount_sqrt'] = np.sqrt(df['amount'])
        
        from scipy.stats import zscore
        df['amount_zscore'] = zscore(df['amount'], nan_policy='omit')
        
        # Amount outlier
        q1 = df['amount'].quantile(0.25)
        q3 = df['amount'].quantile(0.75)
        iqr = q3 - q1
        df['amount_outlier'] = (
            (df['amount'] < (q1 - 1.5 * iqr)) | 
            (df['amount'] > (q3 + 1.5 * iqr))
        ).astype(int)
        
        return df
    
    def save_processed(self, df: pd.DataFrame, filename: str):
        """Save processed data."""
        output_path = self.processed_path / filename
        df.to_csv(output_path, index=False)
        print(f'âœ… Saved to: {output_path}')
        return output_path


def main():
    """Example usage."""
    from pathlib import Path
    
    base_path = Path(__file__).parent.parent.parent / 'data'
    ingester = DataIngester(base_path)
    
    # Load Elliptic
    nodes_df, edges_df = ingester.load_elliptic(sample_frac=0.1)
    ingester.save_processed(nodes_df, 'elliptic_nodes_enhanced.csv')
    
    # Load PaySim
    paysim_df = ingester.load_paysim(sample_frac=0.02)
    ingester.save_processed(paysim_df, 'paysim_sample_enhanced.csv')


if __name__ == '__main__':
    main()


================================================================================
FILE: src/data/sequences.py
================================================================================
"""
FLAG-Finance Sequence Generation Module
Creates transaction sequences for LSTM training
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Tuple, Dict
import pickle
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler, LabelEncoder


class TransactionSequenceGenerator:
    """Generate transaction sequences for LSTM models."""
    
    def __init__(self, sequence_length: int = 10, stride: int = 10, 
                 min_transactions: int = 3, mode: str = 'auto'):
        """
        Args:
            sequence_length: Number of transactions in each sequence
            stride: Step size for sliding window (10 = no overlap)
            min_transactions: Minimum transactions per user (for user-based mode)
            mode: 'user', 'temporal', or 'auto' (auto-detects best approach)
        """
        self.sequence_length = sequence_length
        self.stride = stride
        self.min_transactions = min_transactions
        self.mode = mode
        
        self.feature_scaler = StandardScaler()
        self.categorical_encoders = {}
        self.actual_mode = None
        
    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create advanced temporal and statistical features."""
        df = df.copy()
        
        # Temporal features
        if 'step' in df.columns:
            df['hour'] = df['step'] % 24
            df['day'] = df['step'] // 24
            df['is_weekend'] = (df['day'] % 7 >= 5).astype(int)
            df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)
        
        # Amount features
        if 'amount' in df.columns:
            df['amount_log'] = np.log1p(df['amount'])
            df['amount_sqrt'] = np.sqrt(df['amount'])
        
        # Balance features (PaySim specific)
        if 'oldbalanceOrg' in df.columns:
            df['balance_ratio_orig'] = df['amount'] / (df['oldbalanceOrg'] + 1e-6)
            df['balance_change_orig'] = df['newbalanceOrig'] - df['oldbalanceOrg']
            df['balance_error_orig'] = df['balance_change_orig'] + df['amount']
        
        if 'oldbalanceDest' in df.columns:
            df['balance_ratio_dest'] = df['amount'] / (df['oldbalanceDest'] + 1e-6)
            df['balance_change_dest'] = df['newbalanceDest'] - df['oldbalanceDest']
            df['balance_error_dest'] = df['balance_change_dest'] - df['amount']
        
        return df
    
    def _detect_mode(self, df: pd.DataFrame, user_col: str) -> str:
        """Auto-detect whether to use user-based or temporal mode."""
        if self.mode != 'auto':
            return self.mode
        
        user_tx_counts = df.groupby(user_col).size()
        users_with_enough = (user_tx_counts >= self.sequence_length).sum()
        
        print(f'Mode detection:')
        print(f'  Users with â‰¥{self.sequence_length} transactions: {users_with_enough:,}')
        
        if users_with_enough >= 100:
            print('  â†’ Using USER-BASED mode')
            return 'user'
        else:
            print('  â†’ Using TEMPORAL mode')
            return 'temporal'
    
    def create_sequences_temporal(self, df: pd.DataFrame) -> Tuple[List, List, List]:
        """Create sequences using temporal sliding window."""
        print(f'Creating temporal sequences (length={self.sequence_length}, stride={self.stride})')
        
        df = self.engineer_features(df)
        df = df.sort_values('step').reset_index(drop=True)
        
        # Select features
        feature_cols = [
            'amount', 'amount_log', 'amount_sqrt',
            'hour', 'day', 'is_weekend', 'is_night',
            'oldbalanceOrg', 'newbalanceOrig',
            'oldbalanceDest', 'newbalanceDest',
            'balance_ratio_orig', 'balance_change_orig', 'balance_error_orig',
            'balance_ratio_dest', 'balance_change_dest', 'balance_error_dest'
        ]
        
        # Encode transaction type
        if 'type' in df.columns:
            self.categorical_encoders['type'] = LabelEncoder()
            df['type_encoded'] = self.categorical_encoders['type'].fit_transform(df['type'])
            feature_cols.append('type_encoded')
        
        feature_cols = [col for col in feature_cols if col in df.columns]
        
        sequences = []
        labels = []
        metadata = []
        
        # Sliding window
        for i in tqdm(range(0, len(df) - self.sequence_length + 1, self.stride), 
                     desc='Creating sequences'):
            window = df.iloc[i:i + self.sequence_length]
            
            seq_features = window[feature_cols].values
            seq_label = int(window['isFraud'].iloc[-1])  # Predict LAST transaction
            
            sequences.append(seq_features)
            labels.append(seq_label)
            metadata.append({
                'start_idx': i,
                'end_idx': i + self.sequence_length,
                'fraud_count': int(window['isFraud'].sum())
            })
        
        print(f'Created {len(sequences):,} sequences')
        print(f'Fraud sequences: {sum(labels):,} ({sum(labels)/len(labels)*100:.2f}%)')
        
        return sequences, labels, metadata
    
    def normalize_sequences(self, sequences: List[np.ndarray], 
                          fit: bool = True) -> np.ndarray:
        """Normalize sequence features."""
        if len(sequences) == 0:
            raise ValueError("Cannot normalize empty sequence list")
        
        all_features = np.vstack(sequences)
        
        if fit:
            self.feature_scaler.fit(all_features)
        
        normalized = []
        for seq in sequences:
            normalized.append(self.feature_scaler.transform(seq))
        
        return np.array(normalized)
    
    def create_sequences_paysim(self, df: pd.DataFrame, 
                               user_col: str = 'nameOrig') -> Tuple[List, List, List]:
        """Main entry point for PaySim sequences."""
        self.actual_mode = self._detect_mode(df, user_col)
        return self.create_sequences_temporal(df)
    
    def save(self, path: Path):
        """Save generator state."""
        state = {
            'sequence_length': self.sequence_length,
            'stride': self.stride,
            'min_transactions': self.min_transactions,
            'mode': self.mode,
            'actual_mode': self.actual_mode,
            'feature_scaler': self.feature_scaler,
            'categorical_encoders': self.categorical_encoders
        }
        with open(path, 'wb') as f:
            pickle.dump(state, f)
        print(f'âœ… Saved to: {path}')
    
    @classmethod
    def load(cls, path: Path):
        """Load generator state."""
        with open(path, 'rb') as f:
            state = pickle.load(f)
        
        generator = cls(
            sequence_length=state['sequence_length'],
            stride=state['stride'],
            min_transactions=state['min_transactions'],
            mode=state.get('mode', 'auto')
        )
        generator.actual_mode = state.get('actual_mode')
        generator.feature_scaler = state['feature_scaler']
        generator.categorical_encoders = state['categorical_encoders']
        
        return generator


def main():
    """Example usage."""
    from pathlib import Path
    
    base_path = Path(__file__).parent.parent.parent / 'data'
    
    # Load PaySim data
    df = pd.read_csv(base_path / 'processed' / 'paysim_sample_enhanced.csv')
    
    # Create generator
    generator = TransactionSequenceGenerator(
        sequence_length=10,
        stride=10,  # No overlap
        mode='auto'
    )
    
    # Generate sequences
    sequences, labels, metadata = generator.create_sequences_paysim(df)
    
    # Normalize
    sequences_normalized = generator.normalize_sequences(sequences, fit=True)
    
    print(f'\nFinal shape: {sequences_normalized.shape}')
    
    # Save generator
    generator.save(base_path / 'processed' / 'sequence_generator.pkl')


if __name__ == '__main__':
    main()


================================================================================
FILE: src/models/fusion.py
================================================================================
"""
Fusion models combining GNN and LSTM embeddings
Implements 4 fusion strategies: Simple, Gated, Attention, CrossModal
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class SimpleFusion(nn.Module):
    """
    Simple concatenation fusion with MLP classifier.
    Baseline fusion approach.
    """
    
    def __init__(
        self,
        gnn_dim: int,
        lstm_dim: int,
        hidden_dim: int = 256,
        dropout: float = 0.3,
        num_classes: int = 2
    ):
        super().__init__()
        
        self.fusion = nn.Sequential(
            nn.Linear(gnn_dim + lstm_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, gnn_emb, lstm_emb):
        # Concatenate embeddings
        x = torch.cat([gnn_emb, lstm_emb], dim=1)
        
        # Classification
        out = self.fusion(x)
        
        return out


class GatedFusion(nn.Module):
    """
    Gated fusion with learnable modality weights.
    Learns to weight GNN vs LSTM importance dynamically.
    """
    
    def __init__(
        self,
        gnn_dim: int,
        lstm_dim: int,
        hidden_dim: int = 256,
        dropout: float = 0.3,
        num_classes: int = 2
    ):
        super().__init__()
        
        # Projection layers
        self.gnn_proj = nn.Linear(gnn_dim, hidden_dim)
        self.lstm_proj = nn.Linear(lstm_dim, hidden_dim)
        
        # Gating mechanism
        self.gate = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2),  # 2 weights for GNN and LSTM
            nn.Softmax(dim=1)
        )
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )
        
    def forward(self, gnn_emb, lstm_emb):
        # Project to same dimension
        gnn_proj = F.relu(self.gnn_proj(gnn_emb))
        lstm_proj = F.relu(self.lstm_proj(lstm_emb))
        
        # Compute gate weights
        concat = torch.cat([gnn_proj, lstm_proj], dim=1)
        weights = self.gate(concat)  # (batch, 2)
        
        # Weighted fusion
        fused = weights[:, 0:1] * gnn_proj + weights[:, 1:2] * lstm_proj
        
        # Classification
        out = self.classifier(fused)
        
        return out


class AttentionFusion(nn.Module):
    """
    Multi-head attention fusion between GNN and LSTM modalities.
    Learns fine-grained cross-modal interactions.
    """
    
    def __init__(
        self,
        gnn_dim: int,
        lstm_dim: int,
        hidden_dim: int = 256,
        num_heads: int = 4,
        dropout: float = 0.3,
        num_classes: int = 2
    ):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        
        # Projection layers
        self.gnn_proj = nn.Linear(gnn_dim, hidden_dim)
        self.lstm_proj = nn.Linear(lstm_dim, hidden_dim)
        
        # Multi-head attention
        self.query = nn.Linear(hidden_dim, hidden_dim)
        self.key = nn.Linear(hidden_dim, hidden_dim)
        self.value = nn.Linear(hidden_dim, hidden_dim)
        
        self.dropout = nn.Dropout(dropout)
        
        # Output projection
        self.out_proj = nn.Linear(hidden_dim, hidden_dim)
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )
        
    def forward(self, gnn_emb, lstm_emb):
        batch_size = gnn_emb.size(0)
        
        # Project embeddings
        gnn_proj = self.gnn_proj(gnn_emb)
        lstm_proj = self.lstm_proj(lstm_emb)
        
        # Stack as sequence: [GNN, LSTM]
        x = torch.stack([gnn_proj, lstm_proj], dim=1)  # (batch, 2, hidden)
        
        # Multi-head attention
        Q = self.query(x).view(batch_size, 2, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(batch_size, 2, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(batch_size, 2, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        attn_output = torch.matmul(attn_weights, V)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, 2, self.hidden_dim)
        
        attn_output = self.out_proj(attn_output)
        
        # Concatenate attended features
        fused = torch.cat([attn_output[:, 0], attn_output[:, 1]], dim=1)
        
        # Classification
        out = self.classifier(fused)
        
        return out


class CrossModalFusion(nn.Module):
    """
    Cross-modal fusion with bi-directional multi-head attention.
    Matches the trained checkpoint architecture.
    """
    def __init__(self, gnn_dim: int, lstm_dim: int, hidden_dim: int = 256, 
                 num_heads: int = 4, num_classes: int = 2, dropout: float = 0.4):
        super().__init__()
        
        # Projection layers with LayerNorm
        self.gnn_proj = nn.Sequential(
            nn.Linear(gnn_dim, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        self.lstm_proj = nn.Sequential(
            nn.Linear(lstm_dim, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )
        
        # Bi-directional multi-head attention
        self.gnn_to_lstm_attn = nn.MultiheadAttention(
            hidden_dim, num_heads=num_heads, dropout=dropout, batch_first=True
        )
        
        self.lstm_to_gnn_attn = nn.MultiheadAttention(
            hidden_dim, num_heads=num_heads, dropout=dropout, batch_first=True
        )
        
        # Layer normalization
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        
        # Classifier
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout * 0.7),
            nn.Linear(hidden_dim // 2, num_classes)
        )
    
    def forward(self, gnn_emb, lstm_emb):
        # Project embeddings
        gnn_proj = self.gnn_proj(gnn_emb).unsqueeze(1)  # [B, 1, hidden_dim]
        lstm_proj = self.lstm_proj(lstm_emb).unsqueeze(1)  # [B, 1, hidden_dim]
        
        # Bi-directional cross-attention
        gnn_attended, _ = self.gnn_to_lstm_attn(gnn_proj, lstm_proj, lstm_proj)
        gnn_attended = self.norm1(gnn_attended.squeeze(1) + gnn_proj.squeeze(1))
        
        lstm_attended, _ = self.lstm_to_gnn_attn(lstm_proj, gnn_proj, gnn_proj)
        lstm_attended = self.norm2(lstm_attended.squeeze(1) + lstm_proj.squeeze(1))
        
        # Concatenate and classify
        fused = torch.cat([gnn_attended, lstm_attended], dim=1)
        return self.classifier(fused)



def create_fusion_model(
    model_name: str,
    gnn_dim: int,
    lstm_dim: int,
    hidden_dim: int = 256,
    num_classes: int = 2,
    **kwargs
) -> nn.Module:
    """
    Factory function for fusion models.
    
    Args:
        model_name: 'simple', 'gated', 'attention', or 'crossmodal'
        gnn_dim: GNN embedding dimension
        lstm_dim: LSTM embedding dimension
        hidden_dim: Hidden layer size
        num_classes: Output classes
        **kwargs: Additional arguments
        
    Returns:
        Fusion model instance
    """
    models = {
        'simple': SimpleFusion,
        'gated': GatedFusion,
        'attention': AttentionFusion,
        'crossmodal': CrossModalFusion
    }
    
    if model_name.lower() not in models:
        raise ValueError(f"Unknown model: {model_name}. Choose from {list(models.keys())}")
    
    model_class = models[model_name.lower()]
    
    return model_class(
        gnn_dim=gnn_dim,
        lstm_dim=lstm_dim,
        hidden_dim=hidden_dim,
        num_classes=num_classes,
        **kwargs
    )


if __name__ == '__main__':
    # Test all fusion models
    gnn_emb = torch.randn(32, 384)
    lstm_emb = torch.randn(32, 256)
    
    for model_name in ['simple', 'gated', 'attention', 'crossmodal']:
        model = create_fusion_model(
            model_name=model_name,
            gnn_dim=384,
            lstm_dim=256,
            hidden_dim=256
        )
        
        with torch.no_grad():
            out = model(gnn_emb, lstm_emb)
        
        print(f'âœ… {model_name.upper()}: {out.shape} | Params: {sum(p.numel() for p in model.parameters()):,}')


================================================================================
FILE: src/models/gnn.py
================================================================================
"""
Graph Neural Network architectures for fraud detection
Implements GraphSAGE, GAT, and Hybrid models with deep architectures
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv, GATConv, BatchNorm, global_mean_pool
from torch_geometric.data import Data


class GraphSAGE(nn.Module):
    """
    GraphSAGE model with multiple layers and residual connections.
    
    Args:
        in_channels: Input feature dimension
        hidden_channels: Hidden layer dimension
        num_layers: Number of GraphSAGE layers
        dropout: Dropout probability
        num_classes: Output classes (2 for binary)
    """
    
    def __init__(
        self,
        in_channels: int,
        hidden_channels: int = 256,
        num_layers: int = 3,
        dropout: float = 0.3,
        num_classes: int = 2
    ):
        super().__init__()
        
        self.num_layers = num_layers
        self.dropout = dropout
        
        # Input projection
        self.input_proj = nn.Linear(in_channels, hidden_channels)
        
        # GraphSAGE layers
        self.convs = nn.ModuleList()
        self.bns = nn.ModuleList()
        
        for i in range(num_layers):
            self.convs.append(SAGEConv(hidden_channels, hidden_channels))
            self.bns.append(BatchNorm(hidden_channels))
        
        # Output head
        self.fc1 = nn.Linear(hidden_channels, hidden_channels // 2)
        self.fc2 = nn.Linear(hidden_channels // 2, num_classes)
        
    def forward(self, x, edge_index, return_embeddings=False):
        # Input projection
        x = F.relu(self.input_proj(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        
        # GraphSAGE layers with residual connections
        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):
            x_residual = x
            x = conv(x, edge_index)
            x = bn(x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
            
            # Residual connection
            if i > 0:
                x = x + x_residual
        
        # Store embeddings
        embeddings = x
        
        if return_embeddings:
            return embeddings
        
        # Classification head
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.fc2(x)
        
        return x, embeddings


class DeepSAGE(nn.Module):
    """
    Very deep GraphSAGE with 6-8 layers, skip connections, and LayerNorm.
    Optimized for large graphs with complex patterns.
    """
    
    def __init__(
        self,
        in_channels: int,
        hidden_channels: int = 384,
        num_layers: int = 6,
        dropout: float = 0.4,
        num_classes: int = 2
    ):
        super().__init__()
        
        self.num_layers = num_layers
        self.dropout = dropout
        
        # Input projection
        self.input_proj = nn.Linear(in_channels, hidden_channels)
        self.input_bn = BatchNorm(hidden_channels)
        
        # Deep SAGE layers
        self.convs = nn.ModuleList()
        self.bns = nn.ModuleList()
        
        for _ in range(num_layers):
            self.convs.append(SAGEConv(hidden_channels, hidden_channels))
            self.bns.append(BatchNorm(hidden_channels))
        
        # Skip connection projections (every 2 layers)
        self.skip_projs = nn.ModuleList([
            nn.Linear(hidden_channels, hidden_channels)
            for _ in range(num_layers // 2)
        ])
        
        # Output MLP
        self.mlp = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 2, hidden_channels // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 4, num_classes)
        )
        
    def forward(self, x, edge_index, return_embeddings=False):
        # Input projection
        x = self.input_proj(x)
        x = self.input_bn(x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        
        # Deep layers with skip connections
        skip_idx = 0
        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):
            identity = x
            
            x = conv(x, edge_index)
            x = bn(x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
            
            # Skip connection every 2 layers
            if (i + 1) % 2 == 0 and skip_idx < len(self.skip_projs):
                x = x + self.skip_projs[skip_idx](identity)
                skip_idx += 1
        
        embeddings = x
        
        if return_embeddings:
            return embeddings
        
        # Classification
        out = self.mlp(x)
        
        return out, embeddings


class HybridGNN(nn.Module):
    """
    Hybrid GNN combining GraphSAGE and GAT layers.
    Uses attention for important patterns, aggregation for neighborhoods.
    """
    
    def __init__(
        self,
        in_channels: int,
        hidden_channels: int = 320,
        num_layers: int = 4,
        heads: int = 4,
        dropout: float = 0.35,
        num_classes: int = 2
    ):
        super().__init__()
        
        self.num_layers = num_layers
        self.dropout = dropout
        
        # Input
        self.input_proj = nn.Linear(in_channels, hidden_channels)
        
        # Hybrid layers (alternate SAGE and GAT)
        self.sage_convs = nn.ModuleList()
        self.gat_convs = nn.ModuleList()
        self.bns = nn.ModuleList()
        
        for i in range(num_layers):
            if i % 2 == 0:
                self.sage_convs.append(SAGEConv(hidden_channels, hidden_channels))
            else:
                self.gat_convs.append(
                    GATConv(hidden_channels, hidden_channels // heads, heads=heads, concat=True)
                )
            self.bns.append(BatchNorm(hidden_channels))
        
        # Output
        self.classifier = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 2, num_classes)
        )
        
    def forward(self, x, edge_index, return_embeddings=False):
        x = F.relu(self.input_proj(x))
        x = F.dropout(x, p=self.dropout, training=self.training)
        
        sage_idx = 0
        gat_idx = 0
        
        for i, bn in enumerate(self.bns):
            if i % 2 == 0:
                x = self.sage_convs[sage_idx](x, edge_index)
                sage_idx += 1
            else:
                x = self.gat_convs[gat_idx](x, edge_index)
                gat_idx += 1
            
            x = bn(x)
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
        
        embeddings = x
        
        if return_embeddings:
            return embeddings
        
        out = self.classifier(x)
        
        return out, embeddings


def create_gnn_model(
    model_name: str,
    in_channels: int,
    hidden_channels: int = 256,
    num_layers: int = 3,
    num_classes: int = 2,
    **kwargs
) -> nn.Module:
    """
    Factory function for creating GNN models.
    
    Args:
        model_name: 'graphsage', 'deepsage', or 'hybrid'
        in_channels: Input feature dimension
        hidden_channels: Hidden layer size
        num_layers: Number of GNN layers
        num_classes: Output classes
        **kwargs: Additional model-specific arguments
        
    Returns:
        GNN model instance
    """
    models = {
        'graphsage': GraphSAGE,
        'deepsage': DeepSAGE,
        'hybrid': HybridGNN
    }
    
    if model_name.lower() not in models:
        raise ValueError(f"Unknown model: {model_name}. Choose from {list(models.keys())}")
    
    model_class = models[model_name.lower()]
    
    return model_class(
        in_channels=in_channels,
        hidden_channels=hidden_channels,
        num_layers=num_layers,
        num_classes=num_classes,
        **kwargs
    )


if __name__ == '__main__':
    # Test model creation
    model = create_gnn_model(
        model_name='deepsage',
        in_channels=100,
        hidden_channels=384,
        num_layers=6
    )
    
    print(f'âœ… Model created: {model.__class__.__name__}')
    print(f'   Parameters: {sum(p.numel() for p in model.parameters()):,}')
    
    # Test forward pass
    x = torch.randn(1000, 100)
    edge_index = torch.randint(0, 1000, (2, 5000))
    
    with torch.no_grad():
        out, emb = model(x, edge_index)
        print(f'   Output shape: {out.shape}')
        print(f'   Embedding shape: {emb.shape}')


================================================================================
FILE: src/models/lstm_seq.py
================================================================================
"""
LSTM/GRU sequence models for temporal fraud detection
Implements BiLSTM+Attention, ResidualGRU, and LSTM-CNN hybrid
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class BiLSTMAttention(nn.Module):
    """
    Bidirectional LSTM with attention mechanism.
    
    Args:
        input_size: Feature dimension per time step
        hidden_size: LSTM hidden dimension
        num_layers: Number of LSTM layers
        dropout: Dropout probability
        num_classes: Output classes
    """
    
    def __init__(
        self,
        input_size: int,
        hidden_size: int = 128,
        num_layers: int = 3,
        dropout: float = 0.3,
        num_classes: int = 2
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # BiLSTM
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True,
            batch_first=True
        )
        
        # Attention mechanism
        self.attention = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Tanh(),
            nn.Linear(hidden_size, 1)
        )
        
        # Output MLP
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, num_classes)
        )
        
    def forward(self, x, return_embeddings=False):
        # x: (batch, seq_len, features)
        
        # BiLSTM
        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden*2)
        
        # Attention weights
        attn_weights = self.attention(lstm_out)  # (batch, seq_len, 1)
        attn_weights = F.softmax(attn_weights, dim=1)
        
        # Weighted sum
        context = torch.sum(lstm_out * attn_weights, dim=1)  # (batch, hidden*2)
        
        embeddings = context
        
        if return_embeddings:
            return embeddings
        
        # Classification
        out = self.classifier(context)
        
        return out, embeddings


class ResidualGRU(nn.Module):
    """
    GRU with residual connections between layers.
    More efficient than LSTM with similar performance.
    """
    
    def __init__(
        self,
        input_size: int,
        hidden_size: int = 128,
        num_layers: int = 4,
        dropout: float = 0.3,
        num_classes: int = 2
    ):
        super().__init__()
        
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Input projection
        self.input_proj = nn.Linear(input_size, hidden_size)
        
        # GRU layers
        self.gru_layers = nn.ModuleList([
            nn.GRU(
                input_size=hidden_size,
                hidden_size=hidden_size,
                num_layers=1,
                batch_first=True
            )
            for _ in range(num_layers)
        ])
        
        self.dropout = nn.Dropout(dropout)
        
        # Layer normalization
        self.layer_norms = nn.ModuleList([
            nn.LayerNorm(hidden_size)
            for _ in range(num_layers)
        ])
        
        # Output
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, num_classes)
        )
        
    def forward(self, x, return_embeddings=False):
        # Project input
        x = self.input_proj(x)  # (batch, seq_len, hidden)
        
        # Residual GRU layers
        for i, (gru, ln) in enumerate(zip(self.gru_layers, self.layer_norms)):
            residual = x
            
            x, _ = gru(x)
            x = ln(x)
            x = self.dropout(x)
            
            # Residual connection (after first layer)
            if i > 0:
                x = x + residual
        
        # Take last hidden state
        embeddings = x[:, -1, :]  # (batch, hidden)
        
        if return_embeddings:
            return embeddings
        
        # Classification
        out = self.classifier(embeddings)
        
        return out, embeddings


class LSTMCNNHybrid(nn.Module):
    """
    Hybrid model combining LSTM for temporal patterns and CNN for local features.
    
    Architecture:
    - CNN extracts local patterns from sequences
    - LSTM captures long-term dependencies
    - Concatenate both representations
    """
    
    def __init__(
        self,
        input_size: int,
        hidden_size: int = 128,
        num_layers: int = 2,
        dropout: float = 0.3,
        num_classes: int = 2
    ):
        super().__init__()
        
        # LSTM branch
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True,
            batch_first=True
        )
        
        # CNN branch (1D convolutions over time)
        self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveMaxPool1d(1)
        
        # Fusion
        self.fusion = nn.Sequential(
            nn.Linear(hidden_size * 2 + hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, num_classes)
        )
        
    def forward(self, x, return_embeddings=False):
        # x: (batch, seq_len, features)
        
        # LSTM branch
        lstm_out, _ = self.lstm(x)
        lstm_feat = lstm_out[:, -1, :]  # Last hidden state (batch, hidden*2)
        
        # CNN branch
        x_cnn = x.transpose(1, 2)  # (batch, features, seq_len)
        x_cnn = F.relu(self.conv1(x_cnn))
        x_cnn = F.relu(self.conv2(x_cnn))
        cnn_feat = self.pool(x_cnn).squeeze(-1)  # (batch, hidden)
        
        # Concatenate
        embeddings = torch.cat([lstm_feat, cnn_feat], dim=1)
        
        if return_embeddings:
            return embeddings
        
        # Classification
        out = self.fusion(embeddings)
        
        return out, embeddings


def create_lstm_model(
    model_name: str,
    input_size: int,
    hidden_size: int = 128,
    num_layers: int = 3,
    num_classes: int = 2,
    **kwargs
) -> nn.Module:
    """
    Factory function for LSTM models.
    
    Args:
        model_name: 'bilstm', 'resgru', or 'lstm_cnn'
        input_size: Feature dimension per time step
        hidden_size: Hidden layer size
        num_layers: Number of recurrent layers
        num_classes: Output classes
        **kwargs: Additional arguments
        
    Returns:
        LSTM model instance
    """
    models = {
        'bilstm': BiLSTMAttention,
        'resgru': ResidualGRU,
        'lstm_cnn': LSTMCNNHybrid
    }
    
    if model_name.lower() not in models:
        raise ValueError(f"Unknown model: {model_name}. Choose from {list(models.keys())}")
    
    model_class = models[model_name.lower()]
    
    return model_class(
        input_size=input_size,
        hidden_size=hidden_size,
        num_layers=num_layers,
        num_classes=num_classes,
        **kwargs
    )


if __name__ == '__main__':
    # Test model creation
    model = create_lstm_model(
        model_name='bilstm',
        input_size=20,
        hidden_size=128,
        num_layers=3
    )
    
    print(f'âœ… Model created: {model.__class__.__name__}')
    print(f'   Parameters: {sum(p.numel() for p in model.parameters()):,}')
    
    # Test forward pass
    x = torch.randn(32, 10, 20)  # (batch, seq_len, features)
    
    with torch.no_grad():
        out, emb = model(x)
        print(f'   Output shape: {out.shape}')
        print(f'   Embedding shape: {emb.shape}')


================================================================================
FILE: src/rag/llm_prompting.py
================================================================================
"""
LLM-based fraud explanation generation using RAG
Supports multiple LLM backends: OpenAI, HuggingFace, Anthropic
"""

from pathlib import Path
from typing import Dict, List, Optional
import os
import json

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_community.llms import HuggingFaceHub

from src.rag.retriever import FraudCaseRetriever


class FraudExplainer:
    """
    Generate natural language explanations for fraud predictions using RAG.
    """
    
    def __init__(
        self,
        retriever: FraudCaseRetriever,
        llm_provider: str = 'openai',
        model_name: Optional[str] = None,
        temperature: float = 0.3
    ):
        self.retriever = retriever
        self.llm_provider = llm_provider
        self.temperature = temperature
        
        # Initialize LLM
        self.llm = self._init_llm(llm_provider, model_name)
        
        # Create prompt template
        self.prompt_template = self._create_prompt_template()
        
        # Create chain using LCEL (modern approach)
        if self.llm is not None:
            self.chain = (
                self.prompt_template 
                | self.llm 
                | StrOutputParser()
            )
        else:
            self.chain = None
    
    def _init_llm(self, provider: str, model_name: Optional[str]):
        """Initialize LLM based on provider."""
        if provider == 'openai':
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                print('âš ï¸  No OpenAI API key found, using fallback template')
                return None
            
            return ChatOpenAI(
                model=model_name or 'gpt-3.5-turbo',
                temperature=self.temperature,
                api_key=api_key
            )
        
        elif provider == 'anthropic':
            api_key = os.getenv('ANTHROPIC_API_KEY')
            if not api_key:
                print('âš ï¸  No Anthropic API key found, using fallback template')
                return None
            
            return ChatAnthropic(
                model=model_name or 'claude-3-sonnet-20240229',
                temperature=self.temperature,
                api_key=api_key
            )
        
        elif provider == 'huggingface':
            api_key = os.getenv('HUGGINGFACEHUB_API_TOKEN')
            if not api_key:
                print('âš ï¸  No HuggingFace API key found, using fallback template')
                return None
            
            return HuggingFaceHub(
                repo_id=model_name or 'mistralai/Mistral-7B-Instruct-v0.2',
                huggingfacehub_api_token=api_key,
                model_kwargs={'temperature': self.temperature, 'max_length': 512}
            )
        
        else:
            print(f'âš ï¸  Unknown provider: {provider}, using fallback template')
            return None
    
    def _create_prompt_template(self) -> PromptTemplate:
        """Create prompt template for fraud explanation."""
        template = """You are an expert fraud detection analyst. A transaction has been flagged as potentially fraudulent by our AI system.

**Transaction Details:**
{transaction_details}

**Model Prediction:**
- Fraud Probability: {fraud_probability}%
- Prediction: {prediction}

**Similar Historical Cases:**
{similar_cases}

**Task:**
Provide a clear, professional explanation for why this transaction was flagged as fraud. Your explanation should:
1. Reference specific transaction characteristics
2. Compare to similar historical fraud cases
3. Highlight the most suspicious patterns
4. Be concise (2-3 sentences)

**Explanation:**"""

        return PromptTemplate(
            input_variables=['transaction_details', 'fraud_probability', 'prediction', 'similar_cases'],
            template=template
        )
    
    def explain_prediction(
        self,
        transaction: Dict,
        prediction: int,
        fraud_probability: float,  
        confidence: Optional[str] = None,
        top_k: int = 3
    ) -> str:
        """Generate explanation for fraud prediction."""
        
        # Determine confidence if not provided
        if confidence is None:
            if fraud_probability >= 0.8 or fraud_probability <= 0.2:
                confidence = "HIGH"
            elif fraud_probability >= 0.6 or fraud_probability <= 0.4:
                confidence = "MEDIUM"
            else:
                confidence = "LOW"
        
        # Build basic explanation
        tx_type = transaction.get('type', 'UNKNOWN')
        amount = transaction.get('amount', 0.0)
        
        if prediction == 1:
            # Fraud explanation
            signals = []
            if abs(transaction.get('balance_error_orig', 0)) > 0:
                signals.append("origin balance mismatch")
            if abs(transaction.get('balance_error_dest', 0)) > 0:
                signals.append("destination balance mismatch")
            if transaction.get('isFlaggedFraud', 0) == 1:
                signals.append("system flag")
            if transaction.get('is_night', False):
                signals.append("late-night transaction")
            
            if not signals:
                signals.append("pattern anomalies detected by ML models")
            
            explanation = (
                f"This {tx_type} transaction of ${amount:,.2f} is classified as FRAUD "
                f"with {confidence} confidence ({fraud_probability*100:.1f}% probability). "
                f"Key risk indicators: {', '.join(signals)}."
            )
        else:
            # Legitimate explanation
            explanation = (
                f"This {tx_type} transaction of ${amount:,.2f} appears LEGITIMATE "
                f"with {confidence} confidence ({(1-fraud_probability)*100:.1f}% probability). "
                f"Transaction patterns align with normal behavior."
            )
        
        # Add similar cases if retriever available
        if self.retriever:
            try:
                similar_cases = self.retriever.retrieve(transaction, top_k=top_k)
                if similar_cases:
                    explanation += f"\n\nFound {len(similar_cases)} similar historical fraud cases."
            except Exception as e:
                print(f"Retrieval failed: {e}")
        
        return explanation
        """
        Generate explanation for a fraud prediction.
        
        Args:
            transaction: Transaction metadata dictionary
            fraud_probability: Model's fraud probability (0-100)
            prediction: 'FRAUD' or 'LEGIT'
            top_k: Number of similar cases to retrieve
            
        Returns:
            Dictionary with explanation and metadata
        """
        # Retrieve similar cases
        similar_cases = self.retriever.retrieve(transaction, top_k=top_k)
        
        # Format transaction details
        transaction_details = self._format_transaction(transaction)
        
        # Format similar cases
        similar_cases_text = self._format_similar_cases(similar_cases)
        
        # Generate explanation
        if self.chain is not None:
            try:
                explanation = self.chain.invoke({
                    "transaction_details": transaction_details,
                    "fraud_probability": f"{fraud_probability:.2f}",
                    "prediction": prediction,
                    "similar_cases": similar_cases_text
                })
            except Exception as e:
                print(f'âš ï¸  LLM error: {e}')
                explanation = self._generate_template_explanation(
                    transaction, fraud_probability, similar_cases
                )
        else:
            # Fallback to template-based explanation
            explanation = self._generate_template_explanation(
                transaction, fraud_probability, similar_cases
            )
        
        return {
            'explanation': explanation.strip(),
            'fraud_probability': fraud_probability,
            'prediction': prediction,
            'similar_cases': [
                {'case': case, 'similarity': score}
                for case, score in similar_cases
            ],
            'transaction': transaction
        }
    
    def _format_transaction(self, transaction: Dict) -> str:
        """Format transaction details as text."""
        parts = []
        
        if 'amount' in transaction:
            parts.append(f"Amount: ${transaction['amount']:.2f}")
        if 'type' in transaction:
            parts.append(f"Type: {transaction['type']}")
        if 'hour' in transaction:
            parts.append(f"Time: {transaction['hour']}:00")
        if 'out_degree' in transaction:
            parts.append(f"Network connections: {transaction['out_degree']}")
        
        return ", ".join(parts)
    
    def _format_similar_cases(self, similar_cases: List) -> str:
        """Format similar cases as text."""
        if not similar_cases:
            return "No similar cases found."
        
        texts = []
        for i, (case, score) in enumerate(similar_cases, 1):
            case_text = f"Case {i} (similarity: {score:.2f}): "
            if 'amount' in case:
                case_text += f"${case['amount']:.2f} transaction"
            if 'type' in case:
                case_text += f", {case['type']}"
            if 'pattern' in case:
                case_text += f" - {case['pattern']}"
            texts.append(case_text)
        
        return "\n".join(texts)
    
    def _generate_template_explanation(
        self,
        transaction: Dict,
        fraud_probability: float,
        similar_cases: List
    ) -> str:
        """Generate rule-based explanation when LLM is unavailable."""
        reasons = []
        
        # High probability
        if fraud_probability > 80:
            reasons.append("extremely high fraud probability")
        elif fraud_probability > 60:
            reasons.append("high fraud probability")
        
        # Amount-based
        if 'amount' in transaction and transaction['amount'] > 10000:
            reasons.append("unusually large transaction amount")
        
        # Temporal
        if transaction.get('is_night', False):
            reasons.append("transaction occurred during high-risk hours")
        
        # Balance discrepancy
        if 'balance_error_orig' in transaction and abs(transaction['balance_error_orig']) > 100:
            reasons.append("significant balance discrepancy detected")
        
        # Network anomaly
        if 'out_degree' in transaction and transaction['out_degree'] > 10:
            reasons.append("abnormal network activity pattern")
        
        # Similar cases
        if similar_cases and len(similar_cases) > 0:
            avg_similarity = sum(score for _, score in similar_cases) / len(similar_cases)
            if avg_similarity > 0.8:
                reasons.append(f"strong similarity to {len(similar_cases)} known fraud cases")
        
        # Construct explanation
        if reasons:
            explanation = f"This transaction was flagged as fraud due to: {', '.join(reasons)}. "
        else:
            explanation = "This transaction exhibits patterns consistent with fraudulent behavior. "
        
        explanation += f"The model assigns a {fraud_probability:.1f}% fraud probability based on learned patterns from historical data."
        
        return explanation


def generate_batch_explanations(
    explainer: FraudExplainer,
    predictions: List[Dict],
    save_path: Optional[Path] = None
) -> List[Dict]:
    """
    Generate explanations for batch of predictions.
    
    Args:
        explainer: FraudExplainer instance
        predictions: List of prediction dictionaries
        save_path: Optional path to save explanations
        
    Returns:
        List of explanation dictionaries
    """
    print(f'\nðŸ” Generating explanations for {len(predictions)} predictions...')
    
    explanations = []
    
    for pred in predictions:
        explanation = explainer.explain_prediction(
            transaction=pred['transaction'],
            fraud_probability=pred['fraud_probability'],
            prediction=pred['prediction']
        )
        explanations.append(explanation)
    
    print(f'   âœ… Generated {len(explanations)} explanations')
    
    if save_path:
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(save_path, 'w') as f:
            json.dump(explanations, f, indent=2)
        
        print(f'   ðŸ’¾ Saved to {save_path}')
    
    return explanations


if __name__ == '__main__':
    from pathlib import Path
    from src.rag.retriever import build_fraud_case_database
    
    base_path = Path(__file__).parent.parent.parent / 'data'
    
    # Build retriever
    retriever = build_fraud_case_database(base_path, dataset='paysim', n_cases=100)
    
    # Create explainer
    explainer = FraudExplainer(
        retriever=retriever,
        llm_provider='openai',  # Falls back to template if no API key
        temperature=0.3
    )
    
    # Test explanation
    test_transaction = {
        'amount': 8500.0,
        'type': 'TRANSFER',
        'hour': 2,
        'is_night': True,
        'balance_error_orig': 1200.0
    }
    
    result = explainer.explain_prediction(
        transaction=test_transaction,
        fraud_probability=92.5,
        prediction='FRAUD'
    )
    
    print(f'\nðŸ“ Explanation:')
    print(result['explanation'])


================================================================================
FILE: src/rag/retriever.py
================================================================================
"""
Fixed RAG retrieval system for fraud case similarity search
Handles tuple indexing errors and backward compatibility
"""

from pathlib import Path
from typing import Dict, List, Optional, Tuple
import json
import pickle

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss


class FraudCaseRetriever:
    """
    Vector-based retrieval system for fraud cases.
    Fixed version with proper error handling.
    """
    
    def __init__(
        self,
        embedding_model: str = 'all-MiniLM-L6-v2',
        dimension: int = 384,
        index_type: str = 'flat'
    ):
        self.embedding_model_name = embedding_model
        self.dimension = dimension
        self.index_type = index_type
        
        # Initialize embedding model
        print(f'ðŸ“¦ Loading embedding model: {embedding_model}...')
        self.encoder = SentenceTransformer(embedding_model)
        
        # Initialize FAISS index
        if index_type == 'flat':
            self.index = faiss.IndexFlatL2(dimension)
        elif index_type == 'ivf':
            quantizer = faiss.IndexFlatL2(dimension)
            self.index = faiss.IndexIVFFlat(quantizer, dimension, 100)
        else:
            raise ValueError(f"Unknown index type: {index_type}")
        
        self.cases: List[Dict] = []
        self.case_texts: List[str] = []
    
    def create_case_text(self, case: Dict) -> str:
        """
        Convert fraud case metadata to text description.
        
        Args:
            case: Dictionary with transaction metadata
            
        Returns:
            Text description of the case
        """
        text_parts = []
        
        # Amount
        if 'amount' in case:
            text_parts.append(f"Transaction amount: ${case['amount']:.2f}")
        
        # Transaction type
        if 'type' in case:
            text_parts.append(f"Type: {case['type']}")
        
        # Temporal features
        if 'hour' in case:
            text_parts.append(f"Hour: {case['hour']}")
        if 'is_night' in case and case['is_night']:
            text_parts.append("Occurred at night")
        
        # Balance features
        if 'balance_error_orig' in case:
            text_parts.append(f"Balance discrepancy: ${abs(case['balance_error_orig']):.2f}")
        
        # Graph features
        if 'out_degree' in case:
            text_parts.append(f"Outgoing connections: {case['out_degree']}")
        if 'in_degree' in case:
            text_parts.append(f"Incoming connections: {case['in_degree']}")
        
        # Fraud indicator
        if 'is_fraud' in case:
            text_parts.append(f"Fraud status: {'Fraudulent' if case['is_fraud'] else 'Legitimate'}")
        
        # Pattern description
        if 'pattern' in case:
            text_parts.append(f"Pattern: {case['pattern']}")
        
        return ". ".join(text_parts) + "."
    
    def build_index(self, cases: List[Dict], save_path: Optional[Path] = None):
        """
        Build FAISS index from fraud cases.
        
        Args:
            cases: List of case dictionaries
            save_path: Optional path to save index
        """
        print(f'\nðŸ”¨ Building FAISS index from {len(cases)} cases...')
        
        self.cases = cases
        
        # Convert cases to text
        print('   Creating case texts...')
        self.case_texts = [self.create_case_text(case) for case in cases]
        
        # Generate embeddings
        print('   Encoding cases...')
        embeddings = self.encoder.encode(
            self.case_texts,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # Add to FAISS index
        if self.index_type == 'ivf':
            print('   Training IVF index...')
            self.index.train(embeddings)
        
        print('   Adding vectors to index...')
        self.index.add(embeddings)
        
        print(f'   âœ… Index built with {self.index.ntotal} vectors')
        
        # Save if path provided
        if save_path:
            self.save_index(save_path)
    
    def retrieve(self, query_case: Dict, top_k: int = 3) -> List[Tuple[Dict, float]]:
        """
        Retrieve similar fraud cases.
        
        Args:
            query_case: Query case dictionary
            top_k: Number of similar cases to retrieve
            
        Returns:
            List of (case, similarity_score) tuples
        """
        # Create query text
        query_text = self.create_case_text(query_case)
        
        # Encode query
        query_emb = self.encoder.encode([query_text], convert_to_numpy=True)
        
        # Search
        distances, indices = self.index.search(query_emb, top_k)
        
        # Retrieve cases with scores
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(self.cases):
                similarity_score = 1.0 / (1.0 + dist)  # Convert L2 distance to similarity
                results.append((self.cases[idx], similarity_score))
        
        return results
    
    def save_index(self, save_path: Path):
        """Save FAISS index and metadata."""
        save_path = Path(save_path)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # Save FAISS index
        faiss.write_index(self.index, str(save_path / 'index.faiss'))
        
        # Save metadata with proper structure
        metadata = {
            'cases': self.cases,
            'case_texts': self.case_texts,
            'embedding_model': self.embedding_model_name,
            'dimension': self.dimension,
            'index_type': self.index_type,
            'num_cases': len(self.cases)
        }
        
        with open(save_path / 'metadata.pkl', 'wb') as f:
            pickle.dump(metadata, f)
        
        # Also save as JSON for inspection
        json_metadata = {
            'embedding_model': self.embedding_model_name,
            'dimension': self.dimension,
            'index_type': self.index_type,
            'num_cases': len(self.cases),
            'sample_case': self.cases[0] if self.cases else None
        }
        
        with open(save_path / 'info.json', 'w') as f:
            json.dump(json_metadata, f, indent=2)
        
        print(f'   ðŸ’¾ Index saved to {save_path}')
    
    def load_index(self, load_path: Path):
        """
        Load FAISS index and metadata with error handling.
        
        Args:
            load_path: Path to saved index directory
        """
        load_path = Path(load_path)
        
        if not load_path.exists():
            raise FileNotFoundError(f"Index directory not found: {load_path}")
        
        # Load FAISS index
        index_file = load_path / 'index.faiss'
        if not index_file.exists():
            raise FileNotFoundError(f"FAISS index not found: {index_file}")
        
        self.index = faiss.read_index(str(index_file))
        
        # Try to load metadata with fallback
        metadata_file = load_path / 'metadata.pkl'
        legacy_file = load_path / 'index.pkl'  # Old format
        
        metadata = None
        
        # Try new format first
        if metadata_file.exists():
            try:
                with open(metadata_file, 'rb') as f:
                    metadata = pickle.load(f)
                print(f'   âœ… Loaded metadata from {metadata_file}')
            except Exception as e:
                print(f'   âš ï¸  Failed to load new metadata format: {e}')
        
        # Fallback to legacy format
        if metadata is None and legacy_file.exists():
            try:
                with open(legacy_file, 'rb') as f:
                    legacy_data = pickle.load(f)
                
                # Handle different legacy formats
                if isinstance(legacy_data, dict):
                    # Check if it's the old tuple-based format
                    if 'cases' in legacy_data and isinstance(legacy_data['cases'], list):
                        if legacy_data['cases'] and isinstance(legacy_data['cases'][0], tuple):
                            # Old format: list of tuples
                            print('   â„¹ï¸  Converting from legacy tuple format...')
                            cases = []
                            case_texts = []
                            for item in legacy_data['cases']:
                                if isinstance(item, tuple) and len(item) >= 2:
                                    case_dict = item[0] if isinstance(item[0], dict) else {}
                                    case_text = item[1] if len(item) > 1 else ""
                                    cases.append(case_dict)
                                    case_texts.append(case_text)
                                elif isinstance(item, dict):
                                    cases.append(item)
                                    case_texts.append(self.create_case_text(item))
                            
                            metadata = {
                                'cases': cases,
                                'case_texts': case_texts,
                                'embedding_model': legacy_data.get('embedding_model', self.embedding_model_name),
                                'dimension': legacy_data.get('dimension', self.dimension),
                                'index_type': legacy_data.get('index_type', self.index_type)
                            }
                        else:
                            # New dict format
                            metadata = legacy_data
                    else:
                        metadata = legacy_data
                
                print(f'   âœ… Loaded and converted legacy metadata')
                
            except Exception as e:
                print(f'   âš ï¸  Failed to load legacy metadata: {e}')
        
        # If still no metadata, create minimal structure
        if metadata is None:
            print('   âš ï¸  No metadata found, creating minimal structure')
            num_vectors = self.index.ntotal
            self.cases = [{'case_id': i, 'description': f'Case {i}'} for i in range(num_vectors)]
            self.case_texts = [f'Case {i}' for i in range(num_vectors)]
            print(f'   â„¹ï¸  Created {num_vectors} placeholder cases')
            return
        
        # Extract metadata
        self.cases = metadata.get('cases', [])
        self.case_texts = metadata.get('case_texts', [])
        
        # Validate consistency
        if len(self.cases) != self.index.ntotal:
            print(f'   âš ï¸  Warning: {len(self.cases)} cases but {self.index.ntotal} vectors')
            # Pad or truncate
            if len(self.cases) < self.index.ntotal:
                for i in range(len(self.cases), self.index.ntotal):
                    self.cases.append({'case_id': i, 'description': f'Case {i}'})
                    self.case_texts.append(f'Case {i}')
            else:
                self.cases = self.cases[:self.index.ntotal]
                self.case_texts = self.case_texts[:self.index.ntotal]
        
        print(f'   âœ… Loaded index with {self.index.ntotal} vectors and {len(self.cases)} cases')


def build_fraud_case_database(
    data_path: Path,
    dataset: str = 'paysim',
    n_cases: int = 1000,
    save: bool = True
) -> FraudCaseRetriever:
    """
    Build fraud case database from processed data.
    
    Args:
        data_path: Base data directory
        dataset: 'paysim' or 'elliptic'
        n_cases: Number of fraud cases to index
        save: Whether to save the index
        
    Returns:
        FraudCaseRetriever instance
    """
    print(f'\nðŸ—ï¸  Building fraud case database ({dataset})...')
    
    # Load data
    if dataset == 'paysim':
        processed_file = data_path / 'processed' / 'paysim_sample_enhanced.csv'
        if not processed_file.exists():
            processed_file = data_path / 'processed' / 'paysim_data_enhanced.csv'
        
        if not processed_file.exists():
            raise FileNotFoundError(f"PaySim data not found at {processed_file}")
        
        df = pd.read_csv(processed_file)
        fraud_df = df[df['isFraud'] == 1].head(n_cases)
        
        cases = []
        for _, row in fraud_df.iterrows():
            case = {
                'amount': float(row['amount']),
                'type': str(row['type']),
                'hour': int(row.get('hour', 0)),
                'is_night': bool(row.get('is_night', False)),
                'balance_error_orig': float(row.get('balance_error_orig', 0)),
                'is_fraud': True,
                'pattern': 'suspicious_transaction'
            }
            cases.append(case)
    
    elif dataset == 'elliptic':
        processed_file = data_path / 'processed' / 'elliptic_nodes_enhanced.csv'
        
        if not processed_file.exists():
            raise FileNotFoundError(f"Elliptic data not found at {processed_file}")
        
        df = pd.read_csv(processed_file)
        fraud_df = df[df['class'] == 2].head(n_cases)
        
        cases = []
        for _, row in fraud_df.iterrows():
            case = {
                'amount': float(row.get('feat_1', 0)),
                'out_degree': int(row.get('out_degree', 0)),
                'in_degree': int(row.get('in_degree', 0)),
                'time_step': int(row.get('time_step', 0)),
                'is_fraud': True,
                'pattern': 'illicit_bitcoin_transaction'
            }
            cases.append(case)
    
    else:
        raise ValueError(f"Unknown dataset: {dataset}")
    
    # Build retriever
    retriever = FraudCaseRetriever()
    retriever.build_index(cases)
    
    if save:
        save_path = data_path / 'vector_db' / 'fraud_cases_faiss'
        retriever.save_index(save_path)
    
    return retriever


if __name__ == '__main__':
    from pathlib import Path
    
    # Example: Build database
    base_path = Path(__file__).parent.parent.parent / 'data'
    
    print("Building fraud case database...")
    print("This will create a vector database for RAG explanations.")
    print()
    
    try:
        retriever = build_fraud_case_database(
            data_path=base_path,
            dataset='paysim',
            n_cases=500
        )
        
        # Test retrieval
        test_case = {
            'amount': 5000.0,
            'type': 'TRANSFER',
            'hour': 3,
            'is_night': True,
            'balance_error_orig': 500.0
        }
        
        print('\nðŸ” Testing retrieval...')
        results = retriever.retrieve(test_case, top_k=3)
        
        print(f'\nRetrieved {len(results)} similar cases:')
        for i, (case, score) in enumerate(results, 1):
            print(f'   {i}. Score: {score:.3f} | Amount: ${case.get("amount", 0):.2f}')
        
        print('\nâœ… Database built successfully!')
        
    except FileNotFoundError as e:
        print(f'\nâŒ Error: {e}')
        print('\nPlease ensure you have processed the PaySim data first.')
        print('Run: python src/data/ingest.py')

