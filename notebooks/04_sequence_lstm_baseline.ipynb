{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d64e666",
   "metadata": {},
   "source": [
    "#  Notebook 04: LSTM Sequence Models for Fraud Detection\n",
    "\n",
    "##  Overview\n",
    "\n",
    "This notebook implements advanced sequential models for fraud detection using temporal transaction patterns.\n",
    "\n",
    "###  Objectives\n",
    "\n",
    "1. **Sequence Generation**: Create transaction sequences with temporal windowing\n",
    "2. **Model Architectures**: Build 3 advanced LSTM/GRU models\n",
    "3. **Training Pipeline**: Mixed precision training with class balancing\n",
    "4. **Embedding Extraction**: Generate embeddings for fusion model (Notebook 05)\n",
    "5. **Performance Analysis**: Comprehensive evaluation and visualization\n",
    "\n",
    "###  Model Architectures\n",
    "\n",
    "| Model | Description | Key Features |\n",
    "|-------|-------------|--------------|\n",
    "| **BiLSTM + Attention** | Bidirectional LSTM with attention mechanism | Multi-layer, attention pooling, residual connections |\n",
    "| **Residual GRU** | Deep GRU with skip connections | 4 layers, layer normalization, temporal pooling |\n",
    "| **LSTM-CNN Hybrid** | Parallel LSTM and CNN paths | Multi-scale convolutions, feature fusion |\n",
    "\n",
    "###  Expected Results\n",
    "\n",
    "- **Accuracy**: 85-92%\n",
    "- **F1 Score**: 0.65-0.80\n",
    "- **AUC**: 0.85-0.92\n",
    "\n",
    "### Integration\n",
    "\n",
    "- **Input**: PaySim processed data from Notebook 01\n",
    "- **Output**: Trained models + embeddings for Notebook 05 (Fusion Model)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64001a7f",
   "metadata": {},
   "source": [
    "## 1Ô∏è Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d6bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix,\n",
    "    classification_report, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'CUDA version: {torch.version.cuda}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612fbfd6",
   "metadata": {},
   "source": [
    "## 2 Path Configuration and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ca780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "BASE_PATH = Path(r'C:\\Users\\youss\\Downloads\\Flag_finance\\data')\n",
    "PROCESSED_PATH = BASE_PATH / 'processed'\n",
    "MODELS_PATH = BASE_PATH / 'models'\n",
    "RESULTS_PATH = BASE_PATH / 'results'\n",
    "\n",
    "# Ensure directories exist\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'‚úÖ Using device: {device}')\n",
    "print(f'üìÅ Data path: {BASE_PATH}')\n",
    "print(f'üìÅ Models path: {MODELS_PATH}')\n",
    "print(f'üìÅ Results path: {RESULTS_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b718b0",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Transaction Sequence Generator\n",
    "\n",
    "This class handles:\n",
    "- **Temporal windowing** with configurable stride\n",
    "- **Feature engineering** (17+ features per time step)\n",
    "- **User-level aggregation** for realistic sequences\n",
    "- **Normalization** and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionSequenceGenerator:\n",
    "    \"\"\"\n",
    "    Generate transaction sequences for LSTM training.\n",
    "    \n",
    "    Features:\n",
    "    - Temporal windowing with sliding window\n",
    "    - User-level aggregation\n",
    "    - Advanced feature engineering\n",
    "    - Balanced sampling for training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 sequence_length: int = 10,\n",
    "                 stride: int = 5,\n",
    "                 min_transactions: int = 3):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stride = stride\n",
    "        self.min_transactions = min_transactions\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.categorical_encoders = {}\n",
    "        \n",
    "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create advanced temporal and statistical features.\"\"\"\n",
    "        print('Engineering sequence features...')\n",
    "        \n",
    "        df = df.copy()\n",
    "        \n",
    "        # Temporal features\n",
    "        if 'step' in df.columns:\n",
    "            df['hour'] = df['step'] % 24\n",
    "            df['day'] = df['step'] // 24\n",
    "            df['is_weekend'] = (df['day'] % 7 >= 5).astype(int)\n",
    "            df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
    "        \n",
    "        # Amount features\n",
    "        if 'amount' in df.columns:\n",
    "            df['amount_log'] = np.log1p(df['amount'])\n",
    "            df['amount_sqrt'] = np.sqrt(df['amount'])\n",
    "        \n",
    "        # Balance features (PaySim specific)\n",
    "        if 'oldbalanceOrg' in df.columns:\n",
    "            df['balance_ratio_orig'] = df['amount'] / (df['oldbalanceOrg'] + 1e-6)\n",
    "            df['balance_change_orig'] = df['newbalanceOrig'] - df['oldbalanceOrg']\n",
    "            df['balance_error_orig'] = df['balance_change_orig'] + df['amount']\n",
    "        \n",
    "        if 'oldbalanceDest' in df.columns:\n",
    "            df['balance_ratio_dest'] = df['amount'] / (df['oldbalanceDest'] + 1e-6)\n",
    "            df['balance_change_dest'] = df['newbalanceDest'] - df['oldbalanceDest']\n",
    "            df['balance_error_dest'] = df['balance_change_dest'] - df['amount']\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_sequences_paysim(self, \n",
    "                                df: pd.DataFrame,\n",
    "                                user_col: str = 'nameOrig') -> Tuple[List, List, List]:\n",
    "        \"\"\"Create sequences from PaySim data grouped by user.\"\"\"\n",
    "        print(f'\\nCreating sequences (length={self.sequence_length}, stride={self.stride})...')\n",
    "        \n",
    "        # Engineer features\n",
    "        df = self.engineer_features(df)\n",
    "        \n",
    "        # Sort by user and time\n",
    "        df = df.sort_values([user_col, 'step'])\n",
    "        \n",
    "        # Select features for sequences\n",
    "        feature_cols = [\n",
    "            'amount', 'amount_log', 'amount_sqrt',\n",
    "            'hour', 'day', 'is_weekend', 'is_night',\n",
    "            'oldbalanceOrg', 'newbalanceOrig',\n",
    "            'oldbalanceDest', 'newbalanceDest',\n",
    "            'balance_ratio_orig', 'balance_change_orig', 'balance_error_orig',\n",
    "            'balance_ratio_dest', 'balance_change_dest', 'balance_error_dest'\n",
    "        ]\n",
    "        \n",
    "        # Encode transaction type\n",
    "        if 'type' in df.columns:\n",
    "            self.categorical_encoders['type'] = LabelEncoder()\n",
    "            df['type_encoded'] = self.categorical_encoders['type'].fit_transform(df['type'])\n",
    "            feature_cols.append('type_encoded')\n",
    "        \n",
    "        # Filter available features\n",
    "        feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "        \n",
    "        sequences = []\n",
    "        labels = []\n",
    "        metadata = []\n",
    "        \n",
    "        # Group by user\n",
    "        grouped = df.groupby(user_col)\n",
    "        \n",
    "        for user_id, group in tqdm(grouped, desc='Processing users'):\n",
    "            if len(group) < self.min_transactions:\n",
    "                continue\n",
    "            \n",
    "            group = group.reset_index(drop=True)\n",
    "            \n",
    "            # Sliding window\n",
    "            for i in range(0, len(group) - self.sequence_length + 1, self.stride):\n",
    "                window = group.iloc[i:i + self.sequence_length]\n",
    "                \n",
    "                # Extract features\n",
    "                seq_features = window[feature_cols].values\n",
    "                \n",
    "                # Label: fraud if any transaction in sequence is fraud\n",
    "                seq_label = int(window['isFraud'].max())\n",
    "                \n",
    "                sequences.append(seq_features)\n",
    "                labels.append(seq_label)\n",
    "                metadata.append({\n",
    "                    'user_id': user_id,\n",
    "                    'start_idx': i,\n",
    "                    'end_idx': i + self.sequence_length,\n",
    "                    'fraud_count': int(window['isFraud'].sum())\n",
    "                })\n",
    "        \n",
    "        print(f'Created {len(sequences):,} sequences')\n",
    "        print(f'Fraud sequences: {sum(labels):,} ({sum(labels)/len(labels)*100:.2f}%)')\n",
    "        \n",
    "        return sequences, labels, metadata\n",
    "    \n",
    "    def normalize_sequences(self, sequences: List[np.ndarray], \n",
    "                           fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"Normalize sequence features.\"\"\"\n",
    "        print('Normalizing sequences...')\n",
    "        \n",
    "        # Flatten all sequences for fitting\n",
    "        all_features = np.vstack(sequences)\n",
    "        \n",
    "        if fit:\n",
    "            self.feature_scaler.fit(all_features)\n",
    "        \n",
    "        # Normalize each sequence\n",
    "        normalized = []\n",
    "        for seq in sequences:\n",
    "            normalized.append(self.feature_scaler.transform(seq))\n",
    "        \n",
    "        return np.array(normalized)\n",
    "    \n",
    "    def save(self, path: Path):\n",
    "        \"\"\"Save generator state.\"\"\"\n",
    "        state = {\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'stride': self.stride,\n",
    "            'min_transactions': self.min_transactions,\n",
    "            'feature_scaler': self.feature_scaler,\n",
    "            'categorical_encoders': self.categorical_encoders\n",
    "        }\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(state, f)\n",
    "        print(f'Saved sequence generator to: {path}')\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: Path):\n",
    "        \"\"\"Load generator state.\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "        \n",
    "        generator = cls(\n",
    "            sequence_length=state['sequence_length'],\n",
    "            stride=state['stride'],\n",
    "            min_transactions=state['min_transactions']\n",
    "        )\n",
    "        generator.feature_scaler = state['feature_scaler']\n",
    "        generator.categorical_encoders = state['categorical_encoders']\n",
    "        \n",
    "        print(f'Loaded sequence generator from: {path}')\n",
    "        return generator\n",
    "\n",
    "print('‚úÖ TransactionSequenceGenerator class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8e94d",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ PyTorch Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for transaction sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences: np.ndarray, labels: np.ndarray, \n",
    "                 metadata: Optional[List[Dict]] = None):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.metadata = metadata\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        \"\"\"Compute class weights for balanced training.\"\"\"\n",
    "        labels_np = self.labels.numpy()\n",
    "        class_counts = np.bincount(labels_np)\n",
    "        weights = 1.0 / class_counts\n",
    "        weights = weights / weights.sum() * len(weights)\n",
    "        return torch.FloatTensor(weights)\n",
    "\n",
    "print('‚úÖ SequenceDataset class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe34f9",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Model Architectures\n",
    "\n",
    "### 5.1 Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8e0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Attention mechanism for LSTM outputs.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: (batch, seq_len, hidden_size)\n",
    "        attention_weights = self.attention(lstm_output)  # (batch, seq_len, 1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # Weighted sum\n",
    "        attended = torch.sum(attention_weights * lstm_output, dim=1)  # (batch, hidden_size)\n",
    "        return attended, attention_weights\n",
    "\n",
    "print('‚úÖ AttentionLayer defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4380e",
   "metadata": {},
   "source": [
    "### 5.2 BiLSTM with Attention\n",
    "\n",
    "**Architecture:**\n",
    "- Multi-layer bidirectional LSTM\n",
    "- Attention pooling over time steps\n",
    "- Batch normalization and dropout\n",
    "- Deep classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c30b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM with attention mechanism.\n",
    "    \n",
    "    Features:\n",
    "    - Multi-layer bidirectional LSTM\n",
    "    - Attention pooling over time steps\n",
    "    - Dropout and batch normalization\n",
    "    - Residual connections\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size: int,\n",
    "                 hidden_size: int = 128,\n",
    "                 num_layers: int = 3,\n",
    "                 dropout: float = 0.3,\n",
    "                 num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = AttentionLayer(hidden_size * 2)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.7),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'lstm' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, input_size = x.size()\n",
    "        \n",
    "        # Project each time step\n",
    "        x_proj = self.input_proj(x.view(-1, input_size))\n",
    "        x_proj = x_proj.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x_proj)\n",
    "        \n",
    "        # Attention pooling\n",
    "        attended, attention_weights = self.attention(lstm_out)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(attended)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def extract_embeddings(self, x):\n",
    "        \"\"\"Extract embeddings for fusion model.\"\"\"\n",
    "        batch_size, seq_len, input_size = x.size()\n",
    "        \n",
    "        x_proj = self.input_proj(x.view(-1, input_size))\n",
    "        x_proj = x_proj.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x_proj)\n",
    "        attended, _ = self.attention(lstm_out)\n",
    "        \n",
    "        return attended\n",
    "\n",
    "print('‚úÖ BiLSTMWithAttention defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401bf1d0",
   "metadata": {},
   "source": [
    "### 5.3 Residual GRU\n",
    "\n",
    "**Architecture:**\n",
    "- 4-layer GRU with skip connections\n",
    "- Layer normalization\n",
    "- Temporal max and mean pooling\n",
    "- Residual connections for deep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU with residual connections for deep networks.\n",
    "    \n",
    "    Features:\n",
    "    - Multi-layer GRU with skip connections\n",
    "    - Layer normalization\n",
    "    - Temporal max pooling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int = 128,\n",
    "                 num_layers: int = 4,\n",
    "                 dropout: float = 0.3,\n",
    "                 num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Stacked GRU layers with residual connections\n",
    "        self.gru_layers = nn.ModuleList()\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.gru_layers.append(\n",
    "                nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "            )\n",
    "            self.layer_norms.append(nn.LayerNorm(hidden_size))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Classifier with temporal features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),  # *2 for max+mean pooling\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.GRU):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        nn.init.orthogonal_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        nn.init.zeros_(param)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Project input\n",
    "        x = self.input_proj(x.view(batch_size * seq_len, -1))\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Stacked GRU with residuals\n",
    "        for i, (gru, norm) in enumerate(zip(self.gru_layers, self.layer_norms)):\n",
    "            residual = x\n",
    "            x, _ = gru(x)\n",
    "            x = norm(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            # Residual connection\n",
    "            if i > 0:\n",
    "                x = x + residual * 0.3\n",
    "        \n",
    "        # Temporal pooling\n",
    "        max_pool = torch.max(x, dim=1)[0]\n",
    "        mean_pool = torch.mean(x, dim=1)\n",
    "        pooled = torch.cat([max_pool, mean_pool], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(pooled)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def extract_embeddings(self, x):\n",
    "        \"\"\"Extract embeddings for fusion model.\"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        x = self.input_proj(x.view(batch_size * seq_len, -1))\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        for i, (gru, norm) in enumerate(zip(self.gru_layers, self.layer_norms)):\n",
    "            residual = x\n",
    "            x, _ = gru(x)\n",
    "            x = norm(x)\n",
    "            if i > 0:\n",
    "                x = x + residual * 0.3\n",
    "        \n",
    "        max_pool = torch.max(x, dim=1)[0]\n",
    "        mean_pool = torch.mean(x, dim=1)\n",
    "        return torch.cat([max_pool, mean_pool], dim=1)\n",
    "\n",
    "print('‚úÖ ResidualGRU defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bade3",
   "metadata": {},
   "source": [
    "### 5.4 LSTM-CNN Hybrid\n",
    "\n",
    "**Architecture:**\n",
    "- Parallel LSTM and 1D CNN paths\n",
    "- Multi-scale temporal convolutions (kernel sizes 3, 5, 7)\n",
    "- Feature fusion layer\n",
    "- Combined temporal modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01090b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid LSTM-CNN architecture.\n",
    "    \n",
    "    Features:\n",
    "    - Parallel LSTM and 1D CNN paths\n",
    "    - Feature fusion\n",
    "    - Multi-scale temporal modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 hidden_size: int = 128,\n",
    "                 num_layers: int = 2,\n",
    "                 dropout: float = 0.3,\n",
    "                 num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTM path\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # CNN path (multi-scale)\n",
    "        self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(input_size, hidden_size, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(input_size, hidden_size, kernel_size=7, padding=3)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Fusion layer\n",
    "        fusion_size = hidden_size * 2 + hidden_size * 3  # LSTM (bidir) + 3 CNNs\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_size, hidden_size * 2),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, input_size = x.size()\n",
    "        \n",
    "        # LSTM path\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_max = torch.max(lstm_out, dim=1)[0]\n",
    "        \n",
    "        # CNN path (transpose for Conv1d)\n",
    "        x_cnn = x.transpose(1, 2)  # (batch, input_size, seq_len)\n",
    "        \n",
    "        conv1_out = F.relu(self.bn1(self.conv1(x_cnn)))\n",
    "        conv2_out = F.relu(self.bn2(self.conv2(x_cnn)))\n",
    "        conv3_out = F.relu(self.bn3(self.conv3(x_cnn)))\n",
    "        \n",
    "        # Global max pooling for each conv\n",
    "        conv1_pool = torch.max(conv1_out, dim=2)[0]\n",
    "        conv2_pool = torch.max(conv2_out, dim=2)[0]\n",
    "        conv3_pool = torch.max(conv3_out, dim=2)[0]\n",
    "        \n",
    "        # Fuse all features\n",
    "        fused = torch.cat([lstm_max, conv1_pool, conv2_pool, conv3_pool], dim=1)\n",
    "        fused = self.fusion(fused)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(fused)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def extract_embeddings(self, x):\n",
    "        \"\"\"Extract embeddings for fusion model.\"\"\"\n",
    "        batch_size, seq_len, input_size = x.size()\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_max = torch.max(lstm_out, dim=1)[0]\n",
    "        \n",
    "        x_cnn = x.transpose(1, 2)\n",
    "        conv1_out = F.relu(self.bn1(self.conv1(x_cnn)))\n",
    "        conv2_out = F.relu(self.bn2(self.conv2(x_cnn)))\n",
    "        conv3_out = F.relu(self.bn3(self.conv3(x_cnn)))\n",
    "        \n",
    "        conv1_pool = torch.max(conv1_out, dim=2)[0]\n",
    "        conv2_pool = torch.max(conv2_out, dim=2)[0]\n",
    "        conv3_pool = torch.max(conv3_out, dim=2)[0]\n",
    "        \n",
    "        fused = torch.cat([lstm_max, conv1_pool, conv2_pool, conv3_pool], dim=1)\n",
    "        return self.fusion(fused)\n",
    "\n",
    "print('‚úÖ LSTMCNN defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b940d",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Utilities\n",
    "\n",
    "### 6.1 Focal Loss for Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "print('‚úÖ FocalLoss defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e367bec",
   "metadata": {},
   "source": [
    "### 6.2 Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909796ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, scaler, device):\n",
    "    \"\"\"Train for one epoch with mixed precision.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for sequences, labels in tqdm(loader, desc='Training', leave=False):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        total_loss += loss.item() * sequences.size(0)\n",
    "        \n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for sequences, labels in tqdm(loader, desc='Evaluating', leave=False):\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        total_loss += loss.item() * sequences.size(0)\n",
    "        \n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        preds = probs.argmax(dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, zero_division=0),\n",
    "        'auc': roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.0,\n",
    "        'ap': average_precision_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.0\n",
    "    }\n",
    "    \n",
    "    return metrics, all_preds, all_probs\n",
    "\n",
    "print('‚úÖ Training and evaluation functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7e909",
   "metadata": {},
   "source": [
    "### 6.3 Complete Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d79bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, model_name, \n",
    "                epochs=100, lr=0.001, patience=15, device='cuda'):\n",
    "    \"\"\"Complete training loop with early stopping.\"\"\"\n",
    "    \n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    print(f'Training {model_name}')\n",
    "    print(f'{\"=\"*70}')\n",
    "    \n",
    "    # Get class weights from training dataset\n",
    "    class_weights = train_loader.dataset.get_class_weights().to(device)\n",
    "    criterion = FocalLoss(alpha=class_weights, gamma=2.5)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=20, T_mult=2\n",
    "    )\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_auc': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, scaler, device\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_acc'].append(val_metrics['accuracy'])\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "        history['val_auc'].append(val_metrics['auc'])\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(f'Epoch {epoch:03d}/{epochs} | '\n",
    "                  f'Loss: {train_loss:.4f} | '\n",
    "                  f'Val Acc: {val_metrics[\"accuracy\"]:.4f} | '\n",
    "                  f'Val F1: {val_metrics[\"f1\"]:.4f} | '\n",
    "                  f'Val AUC: {val_metrics[\"auc\"]:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            patience_counter = 0\n",
    "            \n",
    "            checkpoint_path = MODELS_PATH / f'{model_name}_best.pt'\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'val_metrics': val_metrics\n",
    "            }, checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\n‚èπÔ∏è Early stopping at epoch {epoch}')\n",
    "            break\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Load best model and evaluate\n",
    "    checkpoint = torch.load(MODELS_PATH / f'{model_name}_best.pt', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f'\\n‚úÖ Training complete in {train_time:.2f}s')\n",
    "    print(f'Best Val F1: {best_val_f1:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'train_time': train_time,\n",
    "        'best_epoch': epoch - patience_counter,\n",
    "        'best_val_f1': best_val_f1\n",
    "    }\n",
    "\n",
    "print('‚úÖ Complete training function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0026c5c",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Load and Prepare Data\n",
    "\n",
    "Load PaySim data and create transaction sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('LOADING PAYSIM DATA')\n",
    "print('='*70)\n",
    "\n",
    "# Load PaySim data\n",
    "paysim_file = PROCESSED_PATH / 'paysim_sample_enhanced.csv'\n",
    "\n",
    "if not paysim_file.exists():\n",
    "    print(f'‚ö†Ô∏è PaySim file not found: {paysim_file}')\n",
    "    print('Please run notebook 01 (data exploration) first.')\n",
    "else:\n",
    "    # Load data\n",
    "    df = pd.read_csv(paysim_file)\n",
    "    print(f'\\n‚úÖ Loaded PaySim data: {df.shape}')\n",
    "    print(f'Fraud rate: {df[\"isFraud\"].mean()*100:.2f}%')\n",
    "    print(f'\\nColumns: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e2eff",
   "metadata": {},
   "source": [
    "### 7.1 Create Transaction Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sequence generator\n",
    "seq_generator = TransactionSequenceGenerator(\n",
    "    sequence_length=10,\n",
    "    stride=5,\n",
    "    min_transactions=5\n",
    ")\n",
    "\n",
    "# Create sequences\n",
    "sequences, labels, metadata = seq_generator.create_sequences_paysim(df)\n",
    "\n",
    "# Normalize sequences\n",
    "sequences_normalized = seq_generator.normalize_sequences(sequences, fit=True)\n",
    "\n",
    "# Save sequence generator\n",
    "seq_generator.save(PROCESSED_PATH / 'sequence_generator.pkl')\n",
    "\n",
    "print(f'\\nüìä Sequence Statistics:')\n",
    "print(f'   Total sequences: {len(sequences_normalized):,}')\n",
    "print(f'   Sequence shape: {sequences_normalized[0].shape}')\n",
    "print(f'   Fraud sequences: {sum(labels):,} ({sum(labels)/len(labels)*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da86e6",
   "metadata": {},
   "source": [
    "### 7.2 Create Train/Val/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "sequences_np = np.array(sequences_normalized)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "# Temporal split (80/10/10)\n",
    "n_total = len(sequences_np)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_val = int(0.9 * n_total)\n",
    "\n",
    "train_sequences = sequences_np[:n_train]\n",
    "train_labels = labels_np[:n_train]\n",
    "\n",
    "val_sequences = sequences_np[n_train:n_val]\n",
    "val_labels = labels_np[n_train:n_val]\n",
    "\n",
    "test_sequences = sequences_np[n_val:]\n",
    "test_labels = labels_np[n_val:]\n",
    "\n",
    "print(f'Split sizes:')\n",
    "print(f'   Train: {len(train_sequences):,} (fraud: {train_labels.sum():,}, {train_labels.mean()*100:.2f}%)')\n",
    "print(f'   Val:   {len(val_sequences):,} (fraud: {val_labels.sum():,}, {val_labels.mean()*100:.2f}%)')\n",
    "print(f'   Test:  {len(test_sequences):,} (fraud: {test_labels.sum():,}, {test_labels.mean()*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee99c0",
   "metadata": {},
   "source": [
    "### 7.3 Create PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c603ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "train_dataset = SequenceDataset(train_sequences, train_labels)\n",
    "val_dataset = SequenceDataset(val_sequences, val_labels)\n",
    "test_dataset = SequenceDataset(test_sequences, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Data loaders created (batch_size={BATCH_SIZE})')\n",
    "print(f'   Train batches: {len(train_loader)}')\n",
    "print(f'   Val batches: {len(val_loader)}')\n",
    "print(f'   Test batches: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8544dd2",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Train All Models\n",
    "\n",
    "Train the three LSTM architectures and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba016db",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_sequences.shape[2]  # Number of features\n",
    "print(f'Input size (features per time step): {input_size}')\n",
    "\n",
    "# Model configurations\n",
    "models_config = [\n",
    "    {\n",
    "        'name': 'BiLSTM_Attention',\n",
    "        'model': BiLSTMWithAttention(input_size, hidden_size=128, num_layers=3, dropout=0.3),\n",
    "        'lr': 0.001,\n",
    "        'epochs': 100\n",
    "    },\n",
    "    {\n",
    "        'name': 'ResidualGRU',\n",
    "        'model': ResidualGRU(input_size, hidden_size=128, num_layers=4, dropout=0.3),\n",
    "        'lr': 0.001,\n",
    "        'epochs': 100\n",
    "    },\n",
    "    {\n",
    "        'name': 'LSTM_CNN_Hybrid',\n",
    "        'model': LSTMCNN(input_size, hidden_size=128, num_layers=2, dropout=0.3),\n",
    "        'lr': 0.001,\n",
    "        'epochs': 100\n",
    "    }\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for config in models_config:\n",
    "    model_name = config['name']\n",
    "    model = config['model'].to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'\\n{model_name}: {num_params:,} parameters')\n",
    "    \n",
    "    # Train model\n",
    "    result = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model_name=model_name,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['lr'],\n",
    "        patience=15,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    checkpoint = torch.load(MODELS_PATH / f'{model_name}_best.pt', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_metrics, test_preds, test_probs = evaluate(\n",
    "        model, test_loader, \n",
    "        FocalLoss(alpha=train_dataset.get_class_weights().to(device), gamma=2.5),\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    print(f'\\nüìä {model_name} Test Results:')\n",
    "    print(f'   Accuracy:  {test_metrics[\"accuracy\"]:.4f}')\n",
    "    print(f'   Precision: {test_metrics[\"precision\"]:.4f}')\n",
    "    print(f'   Recall:    {test_metrics[\"recall\"]:.4f}')\n",
    "    print(f'   F1 Score:  {test_metrics[\"f1\"]:.4f}')\n",
    "    print(f'   AUC:       {test_metrics[\"auc\"]:.4f}')\n",
    "    print(f'   AP:        {test_metrics[\"ap\"]:.4f}')\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'test_metrics': test_metrics,\n",
    "        'history': result['history'],\n",
    "        'train_time': result['train_time'],\n",
    "        'best_epoch': result['best_epoch'],\n",
    "        'num_parameters': num_params,\n",
    "        'predictions': {\n",
    "            'preds': test_preds,\n",
    "            'probs': test_probs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Clear memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('‚úÖ ALL MODELS TRAINED')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cdb4ec",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Results Analysis and Visualization\n",
    "\n",
    "### 9.1 Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_data = []\n",
    "for model_name, model_results in results.items():\n",
    "    metrics = model_results['test_metrics']\n",
    "    results_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1': metrics['f1'],\n",
    "        'AUC': metrics['auc'],\n",
    "        'AP': metrics['ap'],\n",
    "        'Train Time (s)': model_results['train_time'],\n",
    "        'Epochs': model_results['best_epoch'],\n",
    "        'Parameters': model_results['num_parameters']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df = results_df.sort_values('F1', ascending=False)\n",
    "\n",
    "print('üìä Model Comparison:')\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_model = results_df.iloc[0]['Model']\n",
    "best_f1 = results_df.iloc[0]['F1']\n",
    "print(f'\\nüèÜ Best Model: {best_model} (F1 = {best_f1:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65612b68",
   "metadata": {},
   "source": [
    "### 9.2 Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d605cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training Loss\n",
    "for model_name in results.keys():\n",
    "    axes[0, 0].plot(results[model_name]['history']['train_loss'], \n",
    "                   label=model_name, linewidth=2)\n",
    "axes[0, 0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "for model_name in results.keys():\n",
    "    axes[0, 1].plot(results[model_name]['history']['val_acc'], \n",
    "                   label=model_name, linewidth=2)\n",
    "axes[0, 1].set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation F1\n",
    "for model_name in results.keys():\n",
    "    axes[1, 0].plot(results[model_name]['history']['val_f1'], \n",
    "                   label=model_name, linewidth=2)\n",
    "axes[1, 0].set_title('Validation F1 Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation AUC\n",
    "for model_name in results.keys():\n",
    "    axes[1, 1].plot(results[model_name]['history']['val_auc'], \n",
    "                   label=model_name, linewidth=2)\n",
    "axes[1, 1].set_title('Validation AUC', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('AUC')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / 'lstm_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'‚úÖ Training curves saved to: {RESULTS_PATH / \"lstm_training_curves.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8798d47",
   "metadata": {},
   "source": [
    "### 9.3 Performance Comparison Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cba921",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    axes[0].bar(x + i*width, results_df[metric], width, \n",
    "               label=metric, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Test Set Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x + width * 2)\n",
    "axes[0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Training time\n",
    "axes[1].bar(results_df['Model'], results_df['Train Time (s)'], \n",
    "           color='steelblue', alpha=0.8)\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, v in enumerate(results_df['Train Time (s)']):\n",
    "    axes[1].text(i, v + 0.5, f'{v:.1f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / 'lstm_performance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'‚úÖ Performance comparison saved to: {RESULTS_PATH / \"lstm_performance_comparison.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab085e9",
   "metadata": {},
   "source": [
    "### 9.4 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, model_name in enumerate(results.keys()):\n",
    "    preds = results[model_name]['predictions']['preds']\n",
    "    cm = confusion_matrix(test_labels, preds)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "               xticklabels=['Legit', 'Fraud'],\n",
    "               yticklabels=['Legit', 'Fraud'],\n",
    "               ax=axes[idx])\n",
    "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / 'lstm_confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'‚úÖ Confusion matrices saved to: {RESULTS_PATH / \"lstm_confusion_matrices.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c37d8",
   "metadata": {},
   "source": [
    "## üîü Extract Embeddings for Fusion Model\n",
    "\n",
    "This is a **critical step** for Notebook 05 where we'll combine GNN and LSTM embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e1d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "\n",
    "for model_name in results.keys():\n",
    "    print(f'\\nExtracting embeddings from {model_name}...')\n",
    "    \n",
    "    # Load best model\n",
    "    if model_name == 'BiLSTM_Attention':\n",
    "        model = BiLSTMWithAttention(input_size, hidden_size=128, num_layers=3).to(device)\n",
    "    elif model_name == 'ResidualGRU':\n",
    "        model = ResidualGRU(input_size, hidden_size=128, num_layers=4).to(device)\n",
    "    else:  # LSTM_CNN_Hybrid\n",
    "        model = LSTMCNN(input_size, hidden_size=128, num_layers=2).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(MODELS_PATH / f'{model_name}_best.pt', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract embeddings for all splits\n",
    "    for split_name, loader in [('train', train_loader), \n",
    "                               ('val', val_loader), \n",
    "                               ('test', test_loader)]:\n",
    "        embeddings_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in tqdm(loader, desc=f'Extracting {split_name}', leave=False):\n",
    "                sequences = sequences.to(device)\n",
    "                embeddings = model.extract_embeddings(sequences)\n",
    "                embeddings_list.append(embeddings.cpu())\n",
    "                labels_list.append(labels)\n",
    "        \n",
    "        embeddings_all = torch.cat(embeddings_list).numpy()\n",
    "        labels_all = torch.cat(labels_list).numpy()\n",
    "        \n",
    "        if model_name not in embeddings_dict:\n",
    "            embeddings_dict[model_name] = {}\n",
    "        \n",
    "        embeddings_dict[model_name][split_name] = {\n",
    "            'embeddings': embeddings_all,\n",
    "            'labels': labels_all\n",
    "        }\n",
    "        \n",
    "        print(f'   {split_name}: {embeddings_all.shape}')\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_path = PROCESSED_PATH / 'lstm_embeddings.pkl'\n",
    "with open(embeddings_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_dict, f)\n",
    "\n",
    "print(f'\\n‚úÖ Embeddings saved to: {embeddings_path}')\n",
    "print(f'   These embeddings will be used in Notebook 05 for fusion with GNN embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb4382",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Save Results and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123635e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results summary\n",
    "results_summary = {\n",
    "    'models': results_df.to_dict('records'),\n",
    "    'best_model': {\n",
    "        'name': best_model,\n",
    "        'f1_score': float(best_f1),\n",
    "        'metrics': {k: float(v) for k, v in results[best_model]['test_metrics'].items()}\n",
    "    },\n",
    "    'sequence_config': {\n",
    "        'sequence_length': seq_generator.sequence_length,\n",
    "        'stride': seq_generator.stride,\n",
    "        'input_size': input_size\n",
    "    },\n",
    "    'dataset_info': {\n",
    "        'train_size': len(train_sequences),\n",
    "        'val_size': len(val_sequences),\n",
    "        'test_size': len(test_sequences),\n",
    "        'fraud_rate_train': float(train_labels.mean()),\n",
    "        'fraud_rate_val': float(val_labels.mean()),\n",
    "        'fraud_rate_test': float(test_labels.mean())\n",
    "    }\n",
    "}\n",
    "\n",
    "results_file = RESULTS_PATH / 'lstm_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f'‚úÖ Results saved to: {results_file}')\n",
    "\n",
    "# Save training histories\n",
    "histories = {\n",
    "    model_name: {\n",
    "        k: [float(x) for x in v] \n",
    "        for k, v in result['history'].items()\n",
    "    }\n",
    "    for model_name, result in results.items()\n",
    "}\n",
    "\n",
    "history_file = RESULTS_PATH / 'lstm_training_histories.json'\n",
    "with open(history_file, 'w') as f:\n",
    "    json.dump(histories, f, indent=2)\n",
    "\n",
    "print(f'‚úÖ Training histories saved to: {history_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a03ac",
   "metadata": {},
   "source": [
    "## üìä Final Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dcfad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('üéâ LSTM SEQUENCE MODELS - COMPLETE SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "print(f'\\nüìä Trained Models: {len(results)}')\n",
    "for model_name in results.keys():\n",
    "    metrics = results[model_name]['test_metrics']\n",
    "    print(f'\\n   {model_name}:')\n",
    "    print(f'      Accuracy:  {metrics[\"accuracy\"]:.4f}')\n",
    "    print(f'      Precision: {metrics[\"precision\"]:.4f}')\n",
    "    print(f'      Recall:    {metrics[\"recall\"]:.4f}')\n",
    "    print(f'      F1 Score:  {metrics[\"f1\"]:.4f}')\n",
    "    print(f'      AUC:       {metrics[\"auc\"]:.4f}')\n",
    "    print(f'      AP:        {metrics[\"ap\"]:.4f}')\n",
    "\n",
    "print(f'\\nüèÜ Best Model: {best_model}')\n",
    "print(f'   F1 Score: {best_f1:.4f}')\n",
    "\n",
    "print(f'\\nüìÅ Generated Outputs:')\n",
    "print(f'   ‚úÖ Trained models: {MODELS_PATH}')\n",
    "print(f'   ‚úÖ Training results: {RESULTS_PATH}')\n",
    "print(f'   ‚úÖ Embeddings for fusion: {embeddings_path}')\n",
    "print(f'   ‚úÖ Visualizations: {RESULTS_PATH}')\n",
    "\n",
    "print('\\nüìù Next Steps:')\n",
    "print('   1Ô∏è‚É£ Run Notebook 05: Hybrid Fusion Model')\n",
    "print('   2Ô∏è‚É£ Combine GNN embeddings (from Notebook 03) + LSTM embeddings')\n",
    "print('   3Ô∏è‚É£ Train multi-modal fusion classifier')\n",
    "print('   4Ô∏è‚É£ Compare fusion model with individual models')\n",
    "print('   5Ô∏è‚É£ Achieve state-of-the-art fraud detection performance')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('‚úÖ NOTEBOOK 04 COMPLETE - READY FOR FUSION!')\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
