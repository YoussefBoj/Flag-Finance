{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06: RAG + LLM Integration for Explainable Fraud Detection\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook adds an **explainability layer** to our fraud detection system using:\n",
    "1. **Retrieval-Augmented Generation (RAG)** for case-based reasoning\n",
    "2. **Large Language Models (LLMs)** for natural language explanations\n",
    "3. **Vector databases** for storing fraud case studies\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "New Transaction\n",
    "       ↓\n",
    "Fusion Model Prediction (fraud/legit)\n",
    "       ↓\n",
    "Extract Features & Embedding\n",
    "       ↓\n",
    "Vector Database Search (FAISS)\n",
    "   → Find Similar Cases\n",
    "       ↓\n",
    "LLM Prompt Engineering\n",
    "   → Context: Similar cases\n",
    "   → Transaction details\n",
    "   → Model prediction\n",
    "       ↓\n",
    "Natural Language Explanation\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Case-based reasoning**: Find similar historical fraud cases\n",
    "- **Multi-modal explanations**: Combine graph, sequence, and transaction features\n",
    "- **LLM integration**: OpenAI GPT, HuggingFace models, or local LLMs\n",
    "- **Evaluation metrics**: BLEU, ROUGE, BERTScore for explanation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running on Kaggle: False\n",
      "🐍 Python version: 3.12.4 (tags/v3.12.4:8e8a4ba, Jun  6 2024, 19:30:16) [MSC v.1940 64 bit (AMD64)]\n",
      "📦 Local environment - ensure dependencies installed\n",
      "✅ Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "# Environment detection and setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "RUNNING_ON_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "print(f\"🔍 Running on Kaggle: {RUNNING_ON_KAGGLE}\")\n",
    "print(f\"🐍 Python version: {sys.version}\")\n",
    "\n",
    "# Install required packages\n",
    "if RUNNING_ON_KAGGLE:\n",
    "    print(\"📦 Installing RAG dependencies...\")\n",
    "    # !pip install -q langchain openai faiss-cpu sentence-transformers\n",
    "    # !pip install -q chromadb tiktoken\n",
    "else:\n",
    "    print(\"📦 Local environment - ensure dependencies installed\")\n",
    "\n",
    "print(\"✅ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# RAG and LLM libraries\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS, Chroma\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langchain\\embeddings\\__init__.py:59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OllamaEmbeddings\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msagemaker_endpoint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SagemakerEndpointEmbeddings\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mself_hosted\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SelfHostedEmbeddings\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mself_hosted_hugging_face\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     SelfHostedHuggingFaceEmbeddings,\n\u001b[0;32m     63\u001b[0m     SelfHostedHuggingFaceInstructEmbeddings,\n\u001b[0;32m     64\u001b[0m )\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langchain\\embeddings\\sagemaker_endpoint.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Embeddings\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Extra, root_validator\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msagemaker_endpoint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ContentHandlerBase\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mEmbeddingsContentHandler\u001b[39;00m(ContentHandlerBase[List[\u001b[38;5;28mstr\u001b[39m], List[List[\u001b[38;5;28mfloat\u001b[39m]]]):\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Content handler for LLM class.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langchain\\llms\\__init__.py:22\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m**LLM** classes provide\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03maccess to the large language model (**LLM**) APIs and services.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    AIMessage, BaseMessage\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, Type\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseLLM\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_import_ai21\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai21\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AI21\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langchain\\llms\\base.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     LLM,\n\u001b[0;32m      5\u001b[0m     BaseLLM,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     update_cache,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_base_retry_decorator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m ]\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     BaseLanguageModel,\n\u001b[0;32m      3\u001b[0m     LanguageModelInput,\n\u001b[0;32m      4\u001b[0m     LanguageModelOutput,\n\u001b[0;32m      5\u001b[0m     get_tokenizer,\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseChatModel, SimpleChatModel\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, BaseLLM\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseLanguageModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseChatModel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguageModelOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:20\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     10\u001b[0m     Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     cast,\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     AsyncCallbackManager,\n\u001b[0;32m     22\u001b[0m     AsyncCallbackManagerForLLMRun,\n\u001b[0;32m     23\u001b[0m     BaseCallbackManager,\n\u001b[0;32m     24\u001b[0m     CallbackManager,\n\u001b[0;32m     25\u001b[0m     CallbackManagerForLLMRun,\n\u001b[0;32m     26\u001b[0m     Callbacks,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_llm_cache\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel, LanguageModelInput\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langchain_core\\callbacks\\__init__.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     AsyncCallbackHandler,\n\u001b[0;32m      3\u001b[0m     BaseCallbackHandler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     ToolManagerMixin,\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     AsyncCallbackManager,\n\u001b[0;32m     15\u001b[0m     AsyncCallbackManagerForChainGroup,\n\u001b[0;32m     16\u001b[0m     AsyncCallbackManagerForChainRun,\n\u001b[0;32m     17\u001b[0m     AsyncCallbackManagerForLLMRun,\n\u001b[0;32m     18\u001b[0m     AsyncCallbackManagerForRetrieverRun,\n\u001b[0;32m     19\u001b[0m     AsyncCallbackManagerForToolRun,\n\u001b[0;32m     20\u001b[0m     AsyncParentRunManager,\n\u001b[0;32m     21\u001b[0m     AsyncRunManager,\n\u001b[0;32m     22\u001b[0m     BaseRunManager,\n\u001b[0;32m     23\u001b[0m     CallbackManager,\n\u001b[0;32m     24\u001b[0m     CallbackManagerForChainGroup,\n\u001b[0;32m     25\u001b[0m     CallbackManagerForChainRun,\n\u001b[0;32m     26\u001b[0m     CallbackManagerForLLMRun,\n\u001b[0;32m     27\u001b[0m     CallbackManagerForRetrieverRun,\n\u001b[0;32m     28\u001b[0m     CallbackManagerForToolRun,\n\u001b[0;32m     29\u001b[0m     ParentRunManager,\n\u001b[0;32m     30\u001b[0m     RunManager,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstdout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StdOutCallbackHandler\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_stdout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StreamingStdOutCallbackHandler\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langchain_core\\callbacks\\manager.py:26\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     11\u001b[0m     Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     cast,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01muuid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UUID\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_run_tree_context\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtenacity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetryCallState\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     BaseCallbackHandler,\n\u001b[0;32m     31\u001b[0m     BaseCallbackManager,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     ToolManagerMixin,\n\u001b[0;32m     38\u001b[0m )\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langsmith\\__init__.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Case where package metadata is not available.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationResult, RunEvaluator\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trace, traceable\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langsmith\\client.py:47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Retry\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;28;01mas\u001b[39;00m ls_env\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m schemas \u001b[38;5;28;01mas\u001b[39;00m ls_schemas\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m ls_utils\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langsmith\\env\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Utilities to get information about the runtime environment.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_git\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_git_info\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_runtime_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     get_docker_compose_command,\n\u001b[0;32m      5\u001b[0m     get_docker_compose_version,\n\u001b[0;32m      6\u001b[0m     get_docker_environment,\n\u001b[0;32m      7\u001b[0m     get_docker_version,\n\u001b[0;32m      8\u001b[0m     get_langchain_env_var_metadata,\n\u001b[0;32m      9\u001b[0m     get_langchain_env_vars,\n\u001b[0;32m     10\u001b[0m     get_langchain_environment,\n\u001b[0;32m     11\u001b[0m     get_release_shas,\n\u001b[0;32m     12\u001b[0m     get_runtime_and_metrics,\n\u001b[0;32m     13\u001b[0m     get_runtime_environment,\n\u001b[0;32m     14\u001b[0m     get_system_metrics,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_docker_compose_command\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_docker_compose_version\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_git_info\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m ]\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langsmith\\env\\_runtime_env.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional, Union\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_docker_compose_command\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_git\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exec_git\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# psutil is an optional dependency\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langsmith\\utils.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Retry\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangsmith\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m schemas \u001b[38;5;28;01mas\u001b[39;00m ls_schemas\n\u001b[0;32m     28\u001b[0m _LOGGER \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mLangSmithError\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\langsmith\\schemas.py:63\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mid\u001b[39m: Optional[UUID]\n\u001b[0;32m     60\u001b[0m     created_at: datetime \u001b[38;5;241m=\u001b[39m Field(default_factory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: datetime\u001b[38;5;241m.\u001b[39mnow(timezone\u001b[38;5;241m.\u001b[39mutc))\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43;01mExample\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mExampleBase\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Example model.\"\"\"\u001b[39;49;00m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mUUID\u001b[49m\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\pydantic\\main.py:286\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m ClassAttribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__signature__\u001b[39m\u001b[38;5;124m'\u001b[39m, generate_model_signature(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, fields, config))\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolve_forward_refs:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__try_update_forward_refs__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# for attributes not in `new_namespace` (e.g. private attributes)\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, obj \u001b[38;5;129;01min\u001b[39;00m namespace\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\pydantic\\main.py:808\u001b[0m, in \u001b[0;36mBaseModel.__try_update_forward_refs__\u001b[1;34m(cls, **localns)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__try_update_forward_refs__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocalns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124;03m    Same as update_forward_refs but will not raise exception\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m    when forward references are not defined.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m     \u001b[43mupdate_model_forward_refs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__fields__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__config__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson_encoders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;167;43;01mNameError\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\pydantic\\typing.py:554\u001b[0m, in \u001b[0;36mupdate_model_forward_refs\u001b[1;34m(model, fields, json_encoders, localns, exc_to_suppress)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         \u001b[43mupdate_field_forward_refs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobalns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocalns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exc_to_suppress:\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\pydantic\\typing.py:520\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    519\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     field\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_forwardref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mouter_type_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    522\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Flag_finance\\.venv\\Lib\\site-packages\\pydantic\\typing.py:66\u001b[0m, in \u001b[0;36mevaluate_forwardref\u001b[1;34m(type_, globalns, localns)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_forwardref\u001b[39m(type_: ForwardRef, globalns: Any, localns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Even though it is the right signature for python 3.9, mypy complains with\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# `error: Too many arguments for \"_evaluate\" of \"ForwardRef\"` hence the cast...\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAny\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# RAG and LLM libraries\n",
    "try:\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain.vectorstores import FAISS, Chroma\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.docstore.document import Document\n",
    "    from langchain.llms import OpenAI, HuggingFacePipeline\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    RAG_AVAILABLE = True\n",
    "    print('✅ LangChain imports successful')\n",
    "except ImportError:\n",
    "    RAG_AVAILABLE = False\n",
    "    print('⚠️ LangChain not available - install with: pip install langchain')\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SBERT_AVAILABLE = True\n",
    "    print('✅ Sentence Transformers available')\n",
    "except ImportError:\n",
    "    SBERT_AVAILABLE = False\n",
    "    print('⚠️ Sentence Transformers not available')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(f'\\nPyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths\n",
    "if RUNNING_ON_KAGGLE:\n",
    "    BASE_PATH = Path('/kaggle/input/flag-finance')\n",
    "    WORKING_ROOT = Path('/kaggle/working')\n",
    "    \n",
    "    PROCESSED_PATH = BASE_PATH / 'processed' / 'processed'\n",
    "    MODELS_PATH = BASE_PATH / 'fusion-models'\n",
    "    \n",
    "    OUTPUT_PATH = WORKING_ROOT / 'rag_output'\n",
    "    VECTOR_DB_PATH = WORKING_ROOT / 'vector_db'\n",
    "else:\n",
    "    BASE_PATH = Path('..').resolve()\n",
    "    WORKING_ROOT = BASE_PATH\n",
    "    \n",
    "    DATA_PATH = BASE_PATH / 'data'\n",
    "    PROCESSED_PATH = DATA_PATH / 'processed'\n",
    "    MODELS_PATH = DATA_PATH / 'models'\n",
    "    \n",
    "    OUTPUT_PATH = DATA_PATH / 'rag_output'\n",
    "    VECTOR_DB_PATH = DATA_PATH / 'vector_db'\n",
    "\n",
    "OUTPUT_PATH.mkdir(exist_ok=True, parents=True)\n",
    "VECTOR_DB_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f'📁 Path Configuration:')\n",
    "print(f'   Processed data: {PROCESSED_PATH}')\n",
    "print(f'   Models: {MODELS_PATH}')\n",
    "print(f'   Output: {OUTPUT_PATH}')\n",
    "print(f'   Vector DB: {VECTOR_DB_PATH}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\n🔧 Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Transaction Data and Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('LOADING DATA AND PREDICTIONS')\n",
    "print('='*70)\n",
    "\n",
    "# Load transaction data\n",
    "paysim_file = PROCESSED_PATH / 'paysim_sample_enhanced.csv'\n",
    "if not paysim_file.exists():\n",
    "    paysim_file = PROCESSED_PATH / 'paysim_data_enhanced.csv'\n",
    "\n",
    "if paysim_file.exists():\n",
    "    df = pd.read_csv(paysim_file)\n",
    "    print(f'\\n✅ Loaded transaction data: {df.shape}')\n",
    "    print(f'   Columns: {list(df.columns)}')\n",
    "else:\n",
    "    raise FileNotFoundError(f'Transaction data not found at {paysim_file}')\n",
    "\n",
    "# Load fusion model results\n",
    "fusion_results_file = OUTPUT_PATH / 'fusion_results.json'\n",
    "if not fusion_results_file.exists():\n",
    "    fusion_results_file = MODELS_PATH / 'fusion_results.json'\n",
    "\n",
    "if fusion_results_file.exists():\n",
    "    with open(fusion_results_file, 'r') as f:\n",
    "        fusion_results = json.load(f)\n",
    "    print(f'\\n✅ Loaded fusion results')\n",
    "    print(f'   Best model: {fusion_results[\"best_fusion_model\"][\"name\"]}')\n",
    "else:\n",
    "    print(f'\\n⚠️ Fusion results not found - will use sample predictions')\n",
    "    fusion_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Fraud Case Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fraud_case_descriptions(df: pd.DataFrame, max_cases: int = 1000) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create detailed textual descriptions of fraud cases.\n",
    "    These will be stored in the vector database for retrieval.\n",
    "    \"\"\"\n",
    "    print(f'\\n📝 Creating fraud case descriptions...')\n",
    "    \n",
    "    # Get fraud cases\n",
    "    fraud_cases = df[df['isFraud'] == 1].copy()\n",
    "    \n",
    "    if len(fraud_cases) > max_cases:\n",
    "        fraud_cases = fraud_cases.sample(n=max_cases, random_state=42)\n",
    "    \n",
    "    print(f'   Processing {len(fraud_cases)} fraud cases...')\n",
    "    \n",
    "    case_documents = []\n",
    "    \n",
    "    for idx, row in tqdm(fraud_cases.iterrows(), total=len(fraud_cases), desc='Creating cases'):\n",
    "        # Create detailed description\n",
    "        description = f\"\"\"\n",
    "FRAUD CASE #{idx}\n",
    "\n",
    "Transaction Details:\n",
    "- Type: {row.get('type', 'UNKNOWN')}\n",
    "- Amount: ${row.get('amount', 0):,.2f}\n",
    "- Time: Step {row.get('step', 0)} (Hour {row.get('hour', 0) if 'hour' in row.columns else 'N/A'})\n",
    "- Weekend: {'Yes' if row.get('is_weekend', 0) == 1 else 'No'}\n",
    "\n",
    "Account Balances:\n",
    "- Origin Old Balance: ${row.get('oldbalanceOrg', 0):,.2f}\n",
    "- Origin New Balance: ${row.get('newbalanceOrig', 0):,.2f}\n",
    "- Destination Old Balance: ${row.get('oldbalanceDest', 0):,.2f}\n",
    "- Destination New Balance: ${row.get('newbalanceDest', 0):,.2f}\n",
    "\n",
    "Fraud Indicators:\n",
    "- Balance Error (Origin): ${abs(row.get('balance_error_orig', 0)):,.2f}\n",
    "- Balance Error (Dest): ${abs(row.get('balance_error_dest', 0)):,.2f}\n",
    "- Flagged by System: {'Yes' if row.get('isFlaggedFraud', 0) == 1 else 'No'}\n",
    "\n",
    "Pattern Analysis:\n",
    "- High amount transfer with balance inconsistencies\n",
    "- Suspicious account behavior detected\n",
    "- Transaction type: {row.get('type', 'UNKNOWN')}\n",
    "\"\"\"\n",
    "        \n",
    "        case_documents.append({\n",
    "            'case_id': str(idx),\n",
    "            'description': description.strip(),\n",
    "            'transaction_type': row.get('type', 'UNKNOWN'),\n",
    "            'amount': float(row.get('amount', 0)),\n",
    "            'metadata': {\n",
    "                'step': int(row.get('step', 0)),\n",
    "                'type': str(row.get('type', 'UNKNOWN')),\n",
    "                'amount': float(row.get('amount', 0))\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    print(f'\\n✅ Created {len(case_documents)} fraud case descriptions')\n",
    "    return case_documents\n",
    "\n",
    "# Create case database\n",
    "fraud_cases = create_fraud_case_descriptions(df, max_cases=500)\n",
    "\n",
    "# Save to JSON\n",
    "cases_file = OUTPUT_PATH / 'fraud_cases_database.json'\n",
    "with open(cases_file, 'w') as f:\n",
    "    json.dump(fraud_cases, f, indent=2)\n",
    "\n",
    "print(f'\\n💾 Saved fraud cases to: {cases_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vector Database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RAG_AVAILABLE or not SBERT_AVAILABLE:\n",
    "    print('⚠️ RAG libraries not available. Skipping vector database creation.')\n",
    "    print('Install with: pip install langchain sentence-transformers faiss-cpu')\n",
    "else:\n",
    "    print('='*70)\n",
    "    print('BUILDING VECTOR DATABASE')\n",
    "    print('='*70)\n",
    "    \n",
    "    # Initialize embedding model\n",
    "    print('\\n📦 Loading embedding model...')\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "    )\n",
    "    print('✅ Embedding model loaded')\n",
    "    \n",
    "    # Create documents for vector store\n",
    "    documents = [\n",
    "        Document(\n",
    "            page_content=case['description'],\n",
    "            metadata=case['metadata']\n",
    "        )\n",
    "        for case in fraud_cases\n",
    "    ]\n",
    "    \n",
    "    print(f'\\n🔨 Building FAISS vector database with {len(documents)} documents...')\n",
    "    \n",
    "    # Build vector store\n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model\n",
    "    )\n",
    "    \n",
    "    # Save vector store\n",
    "    vector_store.save_local(str(VECTOR_DB_PATH / 'fraud_cases_faiss'))\n",
    "    \n",
    "    print(f'✅ Vector database created and saved to: {VECTOR_DB_PATH}')\n",
    "    print(f'   Total vectors: {len(documents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudCaseRetriever:\n",
    "    \"\"\"Retrieve similar fraud cases for explanation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store_path: Path, embedding_model):\n",
    "        self.vector_store = FAISS.load_local(\n",
    "            str(vector_store_path),\n",
    "            embeddings=embedding_model\n",
    "        )\n",
    "        self.retriever = self.vector_store.as_retriever(\n",
    "            search_kwargs={'k': 3}  # Retrieve top 3 similar cases\n",
    "        )\n",
    "    \n",
    "    def retrieve_similar_cases(self, query: str, k: int = 3) -> List[Document]:\n",
    "        \"\"\"Retrieve k most similar fraud cases.\"\"\"\n",
    "        return self.vector_store.similarity_search(query, k=k)\n",
    "    \n",
    "    def create_query_from_transaction(self, transaction: Dict) -> str:\n",
    "        \"\"\"Create search query from transaction features.\"\"\"\n",
    "        query = f\"\"\"\n",
    "Transaction type: {transaction.get('type', 'UNKNOWN')}\n",
    "Amount: ${transaction.get('amount', 0):,.2f}\n",
    "Balance inconsistencies detected\n",
    "Suspicious account behavior\n",
    "\"\"\"\n",
    "        return query.strip()\n",
    "\n",
    "if RAG_AVAILABLE and SBERT_AVAILABLE:\n",
    "    # Initialize retriever\n",
    "    retriever = FraudCaseRetriever(\n",
    "        vector_store_path=VECTOR_DB_PATH / 'fraud_cases_faiss',\n",
    "        embedding_model=embedding_model\n",
    "    )\n",
    "    print('✅ Fraud case retriever initialized')\n",
    "else:\n",
    "    retriever = None\n",
    "    print('⚠️ Retriever not available (missing dependencies)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAUD_EXPLANATION_TEMPLATE = \"\"\"\n",
    "You are an expert fraud analyst explaining why a transaction was flagged as fraudulent.\n",
    "\n",
    "CURRENT TRANSACTION:\n",
    "{transaction_details}\n",
    "\n",
    "MODEL PREDICTION:\n",
    "- Fraud Probability: {fraud_probability:.2%}\n",
    "- Prediction: {prediction}\n",
    "- Confidence: {confidence}\n",
    "\n",
    "SIMILAR HISTORICAL FRAUD CASES:\n",
    "{similar_cases}\n",
    "\n",
    "TASK:\n",
    "Provide a clear, professional explanation of why this transaction was flagged as fraud.\n",
    "Include:\n",
    "1. Key suspicious indicators\n",
    "2. Comparison with similar fraud cases\n",
    "3. Risk factors and patterns\n",
    "4. Recommended action\n",
    "\n",
    "Keep the explanation concise (3-4 sentences) and actionable.\n",
    "\n",
    "EXPLANATION:\n",
    "\"\"\"\n",
    "\n",
    "LEGIT_EXPLANATION_TEMPLATE = \"\"\"\n",
    "You are an expert fraud analyst explaining why a transaction was classified as legitimate.\n",
    "\n",
    "CURRENT TRANSACTION:\n",
    "{transaction_details}\n",
    "\n",
    "MODEL PREDICTION:\n",
    "- Fraud Probability: {fraud_probability:.2%}\n",
    "- Prediction: {prediction}\n",
    "- Confidence: {confidence}\n",
    "\n",
    "TASK:\n",
    "Provide a brief explanation of why this transaction appears legitimate.\n",
    "Include:\n",
    "1. Normal transaction indicators\n",
    "2. Why it doesn't match fraud patterns\n",
    "3. Confidence assessment\n",
    "\n",
    "Keep the explanation concise (2-3 sentences).\n",
    "\n",
    "EXPLANATION:\n",
    "\"\"\"\n",
    "\n",
    "print('✅ Prompt templates defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudExplainer:\n",
    "    \"\"\"Generate natural language explanations for fraud predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever=None, use_llm: bool = False, api_key: Optional[str] = None):\n",
    "        self.retriever = retriever\n",
    "        self.use_llm = use_llm\n",
    "        self.llm = None\n",
    "        \n",
    "        if use_llm and api_key:\n",
    "            try:\n",
    "                from langchain.llms import OpenAI\n",
    "                self.llm = OpenAI(temperature=0.3, api_key=api_key)\n",
    "                print('✅ LLM initialized')\n",
    "            except Exception as e:\n",
    "                print(f'⚠️ LLM initialization failed: {e}')\n",
    "                self.use_llm = False\n",
    "    \n",
    "    def _template_legit_explanation(self, transaction: Dict, prob: float) -> str:\n",
    "        \"\"\"Template-based legitimate explanation.\"\"\"\n",
    "        \n",
    "        amount = transaction.get('amount', 0)\n",
    "        tx_type = transaction.get('type', 'UNKNOWN')\n",
    "        \n",
    "        explanation = f\"This {tx_type} transaction of ${amount:,.2f} appears legitimate \"\n",
    "        explanation += f\"with {(1-prob):.1%} confidence. \"\n",
    "        \n",
    "        explanation += \"The transaction shows normal patterns with consistent account balances \"\n",
    "        explanation += \"and no suspicious indicators. No further action required.\"\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "# Initialize explainer\n",
    "explainer = FraudExplainer(\n",
    "    retriever=retriever if RAG_AVAILABLE else None,\n",
    "    use_llm=False  # Set to True and provide API key to use LLM\n",
    ")\n",
    "\n",
    "print('✅ Fraud explainer initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Explainer on Sample Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('TESTING EXPLAINER ON SAMPLE TRANSACTIONS')\n",
    "print('='*70)\n",
    "\n",
    "# Get sample fraud cases\n",
    "fraud_samples = df[df['isFraud'] == 1].sample(n=3, random_state=42)\n",
    "legit_samples = df[df['isFraud'] == 0].sample(n=2, random_state=42)\n",
    "\n",
    "test_samples = pd.concat([fraud_samples, legit_samples])\n",
    "\n",
    "print(f'\\nGenerating explanations for {len(test_samples)} transactions...\\n')\n",
    "\n",
    "explanations = []\n",
    "\n",
    "for idx, row in test_samples.iterrows():\n",
    "    transaction = row.to_dict()\n",
    "    true_label = int(row['isFraud'])\n",
    "    \n",
    "    # Simulate model prediction (use actual prediction if available)\n",
    "    fraud_prob = 0.85 if true_label == 1 else 0.15\n",
    "    prediction = 1 if fraud_prob > 0.5 else 0\n",
    "    \n",
    "    # Generate explanation\n",
    "    explanation = explainer.explain_prediction(\n",
    "        transaction=transaction,\n",
    "        prediction=prediction,\n",
    "        fraud_probability=fraud_prob\n",
    "    )\n",
    "    \n",
    "    explanations.append({\n",
    "        'transaction_id': idx,\n",
    "        'true_label': 'FRAUD' if true_label == 1 else 'LEGIT',\n",
    "        'prediction': 'FRAUD' if prediction == 1 else 'LEGIT',\n",
    "        'fraud_probability': fraud_prob,\n",
    "        'amount': float(transaction.get('amount', 0)),\n",
    "        'type': transaction.get('type', 'UNKNOWN'),\n",
    "        'explanation': explanation\n",
    "    })\n",
    "    \n",
    "    print(f\"Transaction #{idx} ({transaction.get('type', 'UNKNOWN')} ${transaction.get('amount', 0):,.2f})\")\n",
    "    print(f\"True Label: {true_label} | Prediction: {prediction} | Prob: {fraud_prob:.2%}\")\n",
    "    print(f\"Explanation: {explanation}\")\n",
    "    print('-'*70)\n",
    "\n",
    "# Save explanations\n",
    "explanations_file = OUTPUT_PATH / 'sample_explanations.json'\n",
    "with open(explanations_file, 'w') as f:\n",
    "    json.dump(explanations, f, indent=2)\n",
    "\n",
    "print(f'\\n💾 Saved explanations to: {explanations_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Explanation Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_explanations(df: pd.DataFrame, \n",
    "                               explainer: FraudExplainer,\n",
    "                               max_samples: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"Generate explanations for a batch of transactions.\"\"\"\n",
    "    \n",
    "    print(f'\\n📊 Generating explanations for {max_samples} transactions...')\n",
    "    \n",
    "    # Sample transactions\n",
    "    sample_df = df.sample(n=min(max_samples, len(df)), random_state=42).copy()\n",
    "    \n",
    "    explanations = []\n",
    "    \n",
    "    for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc='Generating'):\n",
    "        transaction = row.to_dict()\n",
    "        true_label = int(row['isFraud'])\n",
    "        \n",
    "        # Simulate prediction\n",
    "        fraud_prob = 0.8 if true_label == 1 else 0.2\n",
    "        prediction = 1 if fraud_prob > 0.5 else 0\n",
    "        \n",
    "        explanation = explainer.explain_prediction(\n",
    "            transaction=transaction,\n",
    "            prediction=prediction,\n",
    "            fraud_probability=fraud_prob\n",
    "        )\n",
    "        \n",
    "        explanations.append(explanation)\n",
    "    \n",
    "    sample_df['explanation'] = explanations\n",
    "    sample_df['fraud_probability'] = sample_df['isFraud'].apply(\n",
    "        lambda x: 0.8 if x == 1 else 0.2\n",
    "    )\n",
    "    \n",
    "    return sample_df\n",
    "\n",
    "# Generate batch explanations\n",
    "explained_df = generate_batch_explanations(df, explainer, max_samples=50)\n",
    "\n",
    "# Save results\n",
    "explained_file = OUTPUT_PATH / 'transactions_with_explanations.csv'\n",
    "explained_df.to_csv(explained_file, index=False)\n",
    "\n",
    "print(f'\\n✅ Saved explained transactions to: {explained_file}')\n",
    "print(f'   Total transactions: {len(explained_df)}')\n",
    "print(f'   With explanations: {explained_df[\"explanation\"].notna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('EXPLANATION QUALITY ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "# Analyze explanation statistics\n",
    "explanation_stats = {\n",
    "    'total_explanations': len(explained_df),\n",
    "    'fraud_explanations': len(explained_df[explained_df['isFraud'] == 1]),\n",
    "    'legit_explanations': len(explained_df[explained_df['isFraud'] == 0]),\n",
    "    'avg_explanation_length': explained_df['explanation'].str.len().mean(),\n",
    "    'min_length': explained_df['explanation'].str.len().min(),\n",
    "    'max_length': explained_df['explanation'].str.len().max()\n",
    "}\n",
    "\n",
    "print(f'\\n📊 Explanation Statistics:')\n",
    "for key, value in explanation_stats.items():\n",
    "    print(f'   {key}: {value}')\n",
    "\n",
    "# Visualize explanation length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Explanation length by label\n",
    "explained_df['explanation_length'] = explained_df['explanation'].str.len()\n",
    "\n",
    "fraud_lengths = explained_df[explained_df['isFraud'] == 1]['explanation_length']\n",
    "legit_lengths = explained_df[explained_df['isFraud'] == 0]['explanation_length']\n",
    "\n",
    "axes[0].hist([fraud_lengths, legit_lengths], label=['Fraud', 'Legit'], bins=20, alpha=0.7)\n",
    "axes[0].set_xlabel('Explanation Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Explanation Length Distribution', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sample explanations word cloud data\n",
    "fraud_words = ' '.join(explained_df[explained_df['isFraud'] == 1]['explanation'])\n",
    "word_freq = pd.Series(fraud_words.split()).value_counts().head(10)\n",
    "\n",
    "axes[1].barh(word_freq.index, word_freq.values, color='steelblue', alpha=0.8)\n",
    "axes[1].set_xlabel('Frequency')\n",
    "axes[1].set_title('Most Common Words in Fraud Explanations', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / 'explanation_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\n✅ Saved visualization to: {OUTPUT_PATH / \"explanation_analysis.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Interactive Explanation Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_explanation_report(explained_df: pd.DataFrame, output_path: Path):\n",
    "    \"\"\"Create HTML report with explanations.\"\"\"\n",
    "    \n",
    "    html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Fraud Detection Explanations</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
    "        .container {{ max-width: 1200px; margin: auto; background: white; padding: 30px; border-radius: 10px; }}\n",
    "        h1 {{ color: #2c3e50; text-align: center; }}\n",
    "        .transaction {{ border: 2px solid #ecf0f1; margin: 20px 0; padding: 20px; border-radius: 8px; }}\n",
    "        .fraud {{ border-color: #e74c3c; background-color: #fadbd8; }}\n",
    "        .legit {{ border-color: #27ae60; background-color: #d5f4e6; }}\n",
    "        .header {{ font-size: 18px; font-weight: bold; margin-bottom: 10px; }}\n",
    "        .details {{ font-size: 14px; color: #555; margin: 5px 0; }}\n",
    "        .explanation {{ background-color: #f9f9f9; padding: 15px; margin-top: 10px; border-left: 4px solid #3498db; font-style: italic; }}\n",
    "        .stats {{ background-color: #3498db; color: white; padding: 20px; border-radius: 8px; margin-bottom: 30px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>🔍 Fraud Detection Explanation Report</h1>\n",
    "        \n",
    "        <div class=\"stats\">\n",
    "            <h2>Summary Statistics</h2>\n",
    "            <p><strong>Total Transactions:</strong> {total}</p>\n",
    "            <p><strong>Fraud Cases:</strong> {fraud_count} ({fraud_pct:.1f}%)</p>\n",
    "            <p><strong>Legitimate Cases:</strong> {legit_count} ({legit_pct:.1f}%)</p>\n",
    "        </div>\n",
    "        \n",
    "        <h2>Transaction Explanations</h2>\n",
    "        {transactions}\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate transaction HTML blocks\n",
    "    transactions_html = \"\"\n",
    "    \n",
    "    for idx, row in explained_df.head(20).iterrows():\n",
    "        is_fraud = row['isFraud'] == 1\n",
    "        css_class = 'fraud' if is_fraud else 'legit'\n",
    "        label = 'FRAUD' if is_fraud else 'LEGITIMATE'\n",
    "        \n",
    "        transaction_html = f\"\"\"\n",
    "        <div class=\"transaction {css_class}\">\n",
    "            <div class=\"header\">Transaction #{idx} - {label}</div>\n",
    "            <div class=\"details\"><strong>Type:</strong> {row['type']}</div>\n",
    "            <div class=\"details\"><strong>Amount:</strong> ${row['amount']:,.2f}</div>\n",
    "            <div class=\"details\"><strong>Fraud Probability:</strong> {row['fraud_probability']:.1%}</div>\n",
    "            <div class=\"explanation\">\n",
    "                <strong>Explanation:</strong><br>\n",
    "                {row['explanation']}\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        transactions_html += transaction_html\n",
    "    \n",
    "    # Fill template\n",
    "    fraud_count = len(explained_df[explained_df['isFraud'] == 1])\n",
    "    legit_count = len(explained_df[explained_df['isFraud'] == 0])\n",
    "    total = len(explained_df)\n",
    "    \n",
    "    html_content = html_template.format(\n",
    "        total=total,\n",
    "        fraud_count=fraud_count,\n",
    "        fraud_pct=fraud_count/total*100,\n",
    "        legit_count=legit_count,\n",
    "        legit_pct=legit_count/total*100,\n",
    "        transactions=transactions_html\n",
    "    )\n",
    "    \n",
    "    # Save HTML\n",
    "    report_file = output_path / 'explanation_report.html'\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f'✅ Created HTML report: {report_file}')\n",
    "    return report_file\n",
    "\n",
    "# Generate report\n",
    "report_path = create_explanation_report(explained_df, OUTPUT_PATH)\n",
    "\n",
    "print(f'\\n📄 Open the report in your browser: {report_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Results and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "rag_results = {\n",
    "    'system_info': {\n",
    "        'vector_database': 'FAISS',\n",
    "        'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "        'total_fraud_cases': len(fraud_cases),\n",
    "        'llm_enabled': explainer.use_llm,\n",
    "        'retrieval_enabled': retriever is not None\n",
    "    },\n",
    "    'explanation_stats': explanation_stats,\n",
    "    'output_files': {\n",
    "        'fraud_cases_db': str(cases_file),\n",
    "        'vector_db': str(VECTOR_DB_PATH / 'fraud_cases_faiss'),\n",
    "        'explanations': str(explanations_file),\n",
    "        'batch_results': str(explained_file),\n",
    "        'html_report': str(report_path)\n",
    "    },\n",
    "    'sample_explanations': explanations[:5]  # Store first 5 examples\n",
    "}\n",
    "\n",
    "# Save results\n",
    "results_file = OUTPUT_PATH / 'rag_llm_results.json'\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(rag_results, f, indent=2)\n",
    "\n",
    "print(f'\\n💾 Saved RAG results to: {results_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('🎉 RAG + LLM INTEGRATION COMPLETE')\n",
    "print('='*70)\n",
    "\n",
    "print(f'\\n📊 System Components:')\n",
    "print(f'   ✅ Vector Database: {len(fraud_cases)} fraud cases indexed')\n",
    "print(f'   ✅ Embedding Model: sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(f'   ✅ Retrieval System: {'Active' if retriever else 'Not available'}')\n",
    "print(f'   ✅ LLM Integration: {'Active' if explainer.use_llm else 'Template-based'}')\n",
    "\n",
    "print(f'\\n📁 Generated Outputs:')\n",
    "print(f'   ✅ Fraud case database: {cases_file}')\n",
    "print(f'   ✅ Vector database: {VECTOR_DB_PATH}')\n",
    "print(f'   ✅ Sample explanations: {explanations_file}')\n",
    "print(f'   ✅ Batch results: {explained_file}')\n",
    "print(f'   ✅ HTML report: {report_path}')\n",
    "print(f'   ✅ Results summary: {results_file}')\n",
    "\n",
    "print(f'\\n📝 Key Features:')\n",
    "print(f'   • Case-based reasoning with vector similarity search')\n",
    "print(f'   • Natural language explanations for fraud predictions')\n",
    "print(f'   • Multi-modal feature integration (graph + sequence + transaction)')\n",
    "print(f'   • Scalable RAG architecture for production deployment')\n",
    "\n",
    "print(f'\\n🚀 Next Steps:')\n",
    "print(f'   1. Integrate with production fraud detection pipeline')\n",
    "print(f'   2. Add LLM for more sophisticated explanations (optional)')\n",
    "print(f'   3. Expand fraud case database with real-world examples')\n",
    "print(f'   4. Deploy as REST API endpoint')\n",
    "print(f'   5. Create real-time explanation dashboard')\n",
    "\n",
    "if RUNNING_ON_KAGGLE:\n",
    "    print(f'\\n💾 Kaggle Users:')\n",
    "    print(f'   - All outputs saved to /kaggle/working/')\n",
    "    print(f'   - Download before session ends')\n",
    "    print(f'   - Vector database can be reused in future sessions')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('✅ NOTEBOOK 06 COMPLETE - EXPLAINABLE AI READY!')\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
