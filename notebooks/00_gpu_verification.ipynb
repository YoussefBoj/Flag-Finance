{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd3c154",
   "metadata": {},
   "source": [
    "# GPU Verification and Configuration\n",
    "\n",
    "**Purpose:** Verify GPU setup and optimize configuration for training.\n",
    "\n",
    "**Run this notebook first** to ensure your GPU is properly configured before training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c00adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.11\n",
      "Platform: Windows 10\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch_geometric\n",
    "import platform\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1375fac",
   "metadata": {},
   "source": [
    "## 1. Check PyTorch and CUDA Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d25f1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PYTORCH & CUDA CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "üì¶ PyTorch Version: 2.2.0+cu121\n",
      "üì¶ PyTorch Geometric Version: 2.6.1\n",
      "\n",
      "üéÆ CUDA Available: True\n",
      "   CUDA Version: 12.1\n",
      "   cuDNN Version: 8801\n",
      "   cuDNN Enabled: True\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PYTORCH & CUDA CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüì¶ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"üì¶ PyTorch Geometric Version: {torch_geometric.__version__}\")\n",
    "\n",
    "print(f\"\\nüéÆ CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"   cuDNN Enabled: {torch.backends.cudnn.enabled}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: CUDA not available!\")\n",
    "    print(\"   You can still train models on CPU (slower)\")\n",
    "    print(\"   To enable GPU:\")\n",
    "    print(\"   1. Ensure you have an NVIDIA GPU\")\n",
    "    print(\"   2. Install CUDA-enabled PyTorch\")\n",
    "    print(\"   3. Run: conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad2272",
   "metadata": {},
   "source": [
    "## 2. GPU Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a3660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GPU INFORMATION\n",
      "======================================================================\n",
      "\n",
      "üéÆ Number of GPUs: 1\n",
      "\n",
      "GPU 0:\n",
      "   Name: NVIDIA GeForce RTX 2060\n",
      "   Total Memory: 6.00 GB\n",
      "   Compute Capability: 7.5\n",
      "   Multi Processors: 30\n",
      "   Memory Allocated: 0.00 MB\n",
      "   Memory Cached: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"=\"*70)\n",
    "    print(\"GPU INFORMATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"\\nüéÆ Number of GPUs: {num_gpus}\")\n",
    "    \n",
    "    for i in range(num_gpus):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\nGPU {i}:\")\n",
    "        print(f\"   Name: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"   Total Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"   Compute Capability: {props.major}.{props.minor}\")\n",
    "        print(f\"   Multi Processors: {props.multi_processor_count}\")\n",
    "        \n",
    "        # Memory info\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**2\n",
    "        cached = torch.cuda.memory_reserved(i) / 1024**2\n",
    "        print(f\"   Memory Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"   Memory Cached: {cached:.2f} MB\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU detected. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4dc845",
   "metadata": {},
   "source": [
    "## 3. Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23be00b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEVICE CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Using device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 2060\n",
      "   Memory Available: 6.00 GB\n",
      "\n",
      "   ‚ö° GPU training enabled - expect 5-10x speedup!\n"
     ]
    }
   ],
   "source": [
    "# Set device automatically\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DEVICE CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"\\n   ‚ö° GPU training enabled - expect 5-10x speedup!\")\n",
    "else:\n",
    "    print(\"\\n   üíª CPU training - models will train slower\")\n",
    "    print(\"   Consider using Google Colab for free GPU access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade8b687",
   "metadata": {},
   "source": [
    "## 4. Test GPU Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb11592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GPU COMPUTATION TEST\n",
      "======================================================================\n",
      "\n",
      "Testing matrix multiplication on GPU...\n",
      "   GPU time: 73.48 ms\n",
      "\n",
      "Comparing with CPU...\n",
      "   CPU time: 1610.87 ms\n",
      "\n",
      "üöÄ GPU Speedup: 21.92x faster than CPU\n",
      "\n",
      "‚úÖ GPU computation test passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GPU COMPUTATION TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    import time\n",
    "    \n",
    "    # Test matrix multiplication on GPU\n",
    "    print(\"\\nTesting matrix multiplication on GPU...\")\n",
    "    \n",
    "    size = 5000\n",
    "    x_gpu = torch.randn(size, size).cuda()\n",
    "    y_gpu = torch.randn(size, size).cuda()\n",
    "    \n",
    "    # Warm up\n",
    "    _ = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Time GPU computation\n",
    "    start = time.time()\n",
    "    z_gpu = torch.matmul(x_gpu, y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"   GPU time: {gpu_time*1000:.2f} ms\")\n",
    "    \n",
    "    # Compare with CPU\n",
    "    print(\"\\nComparing with CPU...\")\n",
    "    x_cpu = torch.randn(size, size)\n",
    "    y_cpu = torch.randn(size, size)\n",
    "    \n",
    "    start = time.time()\n",
    "    z_cpu = torch.matmul(x_cpu, y_cpu)\n",
    "    cpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"   CPU time: {cpu_time*1000:.2f} ms\")\n",
    "    \n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\nüöÄ GPU Speedup: {speedup:.2f}x faster than CPU\")\n",
    "    \n",
    "    # Clean up\n",
    "    del x_gpu, y_gpu, z_gpu, x_cpu, y_cpu, z_cpu\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n‚úÖ GPU computation test passed!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping GPU test (no GPU available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37208146",
   "metadata": {},
   "source": [
    "## 5. Test PyTorch Geometric on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2957f976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PYTORCH GEOMETRIC GPU TEST\n",
      "======================================================================\n",
      "\n",
      "Testing GCN on CPU...\n",
      "   Output shape: torch.Size([4, 32])\n",
      "   ‚úÖ CPU test passed\n",
      "\n",
      "Testing GCN on GPU...\n",
      "   Output shape: torch.Size([4, 32])\n",
      "   Output device: cuda:0\n",
      "   ‚úÖ GPU test passed\n",
      "\n",
      "   Max difference CPU vs GPU: 2.38e-07\n",
      "   ‚úÖ Results match!\n",
      "\n",
      "‚úÖ All PyTorch Geometric tests passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PYTORCH GEOMETRIC GPU TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create a small graph\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 2, 3], \n",
    "                          [1, 0, 2, 1, 3, 2]], dtype=torch.long)\n",
    "x = torch.randn(4, 16)  # 4 nodes, 16 features\n",
    "\n",
    "# Create GCN layer\n",
    "conv = GCNConv(16, 32)\n",
    "\n",
    "print(\"\\nTesting GCN on CPU...\")\n",
    "out_cpu = conv(x, edge_index)\n",
    "print(f\"   Output shape: {out_cpu.shape}\")\n",
    "print(\"   ‚úÖ CPU test passed\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nTesting GCN on GPU...\")\n",
    "    \n",
    "    # Move to GPU\n",
    "    x_gpu = x.to(device)\n",
    "    edge_index_gpu = edge_index.to(device)\n",
    "    conv_gpu = conv.to(device)\n",
    "    \n",
    "    out_gpu = conv_gpu(x_gpu, edge_index_gpu)\n",
    "    print(f\"   Output shape: {out_gpu.shape}\")\n",
    "    print(f\"   Output device: {out_gpu.device}\")\n",
    "    print(\"   ‚úÖ GPU test passed\")\n",
    "    \n",
    "    # Verify results match (within floating point precision)\n",
    "    difference = (out_cpu - out_gpu.cpu()).abs().max().item()\n",
    "    print(f\"\\n   Max difference CPU vs GPU: {difference:.2e}\")\n",
    "    \n",
    "    if difference < 1e-4:\n",
    "        print(\"   ‚úÖ Results match!\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Results differ (normal for floating point)\")\n",
    "    \n",
    "    # Clean up\n",
    "    del x_gpu, edge_index_gpu, conv_gpu, out_gpu\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úÖ All PyTorch Geometric tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526d736",
   "metadata": {},
   "source": [
    "## 6. GPU Optimization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a499a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GPU OPTIMIZATION SETTINGS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ cuDNN autotuner enabled\n",
      "   (automatically selects best convolution algorithms)\n",
      "\n",
      "üíæ Memory Management:\n",
      "   - Empty cache on start: torch.cuda.empty_cache()\n",
      "   - Use gradient checkpointing for large models\n",
      "   - Enable mixed precision training with torch.cuda.amp\n",
      "\n",
      "‚ö° Performance Tips:\n",
      "   - Use batch sizes that are multiples of 32\n",
      "   - Pin memory for DataLoader: pin_memory=True\n",
      "   - Use multiple workers: num_workers=4\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GPU OPTIMIZATION SETTINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Enable cuDNN autotuner\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"\\n‚úÖ cuDNN autotuner enabled\")\n",
    "    print(\"   (automatically selects best convolution algorithms)\")\n",
    "    \n",
    "    # Enable TF32 on Ampere GPUs (RTX 30xx, 40xx)\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"\\n‚úÖ TensorFloat-32 (TF32) enabled\")\n",
    "        print(\"   (faster training on Ampere+ GPUs)\")\n",
    "    \n",
    "    # Memory optimization\n",
    "    print(\"\\nüíæ Memory Management:\")\n",
    "    print(\"   - Empty cache on start: torch.cuda.empty_cache()\")\n",
    "    print(\"   - Use gradient checkpointing for large models\")\n",
    "    print(\"   - Enable mixed precision training with torch.cuda.amp\")\n",
    "    \n",
    "    print(\"\\n‚ö° Performance Tips:\")\n",
    "    print(\"   - Use batch sizes that are multiples of 32\")\n",
    "    print(\"   - Pin memory for DataLoader: pin_memory=True\")\n",
    "    print(\"   - Use multiple workers: num_workers=4\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nüíª CPU Mode\")\n",
    "    print(\"   GPU optimizations not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20268b08",
   "metadata": {},
   "source": [
    "## 7. Recommended Settings for Your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1200bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RECOMMENDED TRAINING SETTINGS\n",
      "======================================================================\n",
      "\n",
      "üéÆ Detected: NVIDIA GeForce RTX 2060\n",
      "   Memory: 6.00 GB\n",
      "\n",
      "‚ö†Ô∏è  Limited GPU memory\n",
      "   Recommended settings:\n",
      "   - batch_size = 256\n",
      "   - hidden_channels = 32\n",
      "   - num_layers = 2\n",
      "   - Consider using CPU for some tasks\n",
      "\n",
      "üìä Expected training time (Elliptic dataset):\n",
      "   - All 4 models: ~20-30 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RECOMMENDED TRAINING SETTINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    \n",
    "    print(f\"\\nüéÆ Detected: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory_gb:.2f} GB\")\n",
    "    \n",
    "    # Recommend settings based on memory\n",
    "    if gpu_memory_gb >= 12:\n",
    "        print(\"\\n‚ú® High-end GPU detected!\")\n",
    "        print(\"   Recommended settings:\")\n",
    "        print(\"   - batch_size = 2048\")\n",
    "        print(\"   - hidden_channels = 256\")\n",
    "        print(\"   - num_layers = 4\")\n",
    "        print(\"   - You can train all models simultaneously\")\n",
    "    elif gpu_memory_gb >= 8:\n",
    "        print(\"\\n‚ö° Mid-range GPU - Perfect for this project!\")\n",
    "        print(\"   Recommended settings:\")\n",
    "        print(\"   - batch_size = 1024\")\n",
    "        print(\"   - hidden_channels = 128\")\n",
    "        print(\"   - num_layers = 3\")\n",
    "        print(\"   - Train one model at a time\")\n",
    "    elif gpu_memory_gb >= 6:\n",
    "        print(\"\\nüí™ Entry-level GPU\")\n",
    "        print(\"   Recommended settings:\")\n",
    "        print(\"   - batch_size = 512\")\n",
    "        print(\"   - hidden_channels = 64\")\n",
    "        print(\"   - num_layers = 2\")\n",
    "        print(\"   - Use gradient accumulation if needed\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Limited GPU memory\")\n",
    "        print(\"   Recommended settings:\")\n",
    "        print(\"   - batch_size = 256\")\n",
    "        print(\"   - hidden_channels = 32\")\n",
    "        print(\"   - num_layers = 2\")\n",
    "        print(\"   - Consider using CPU for some tasks\")\n",
    "    \n",
    "    print(\"\\nüìä Expected training time (Elliptic dataset):\")\n",
    "    if gpu_memory_gb >= 8:\n",
    "        print(\"   - All 4 models: ~15-20 minutes\")\n",
    "    else:\n",
    "        print(\"   - All 4 models: ~20-30 minutes\")\n",
    "else:\n",
    "    print(\"\\nüíª CPU Mode\")\n",
    "    print(\"   Expected training time: ~2 hours\")\n",
    "    print(\"   Consider using cloud GPU (Google Colab, AWS, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01696d12",
   "metadata": {},
   "source": [
    "## 8. Memory Monitoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6cef40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory monitoring functions defined:\n",
      "  - print_gpu_memory()  # Show current usage\n",
      "  - clear_gpu_memory()  # Clear cache\n",
      "\n",
      "Testing...\n",
      "GPU Memory:\n",
      "  Allocated: 104.13 MB / 6143.69 MB (1.7%)\n",
      "  Reserved:  118.00 MB\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory():\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**2\n",
    "        \n",
    "        print(f\"GPU Memory:\")\n",
    "        print(f\"  Allocated: {allocated:.2f} MB / {total:.2f} MB ({allocated/total*100:.1f}%)\")\n",
    "        print(f\"  Reserved:  {reserved:.2f} MB\")\n",
    "    else:\n",
    "        print(\"GPU not available\")\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU cache.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úÖ GPU cache cleared\")\n",
    "    else:\n",
    "        print(\"GPU not available\")\n",
    "\n",
    "# Test the functions\n",
    "print(\"Memory monitoring functions defined:\")\n",
    "print(\"  - print_gpu_memory()  # Show current usage\")\n",
    "print(\"  - clear_gpu_memory()  # Clear cache\")\n",
    "print(\"\\nTesting...\")\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8fd1c",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f972c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFICATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚úì Completed checks:\n",
      "  ‚úÖ PyTorch installed\n",
      "  ‚úÖ PyTorch Geometric installed\n",
      "  ‚úÖ CUDA available\n",
      "  ‚úÖ GPU detected\n",
      "  ‚úÖ GPU computation works\n",
      "  ‚úÖ PyG GPU support works\n",
      "\n",
      "üéÆ GPU: NVIDIA GeForce RTX 2060\n",
      "   Memory: 6.00 GB\n",
      "\n",
      "‚úÖ You're ready to train on GPU!\n",
      "\n",
      "üìù Next steps:\n",
      "   1. Run 01_data_exploration.ipynb\n",
      "   2. Run 02_graph_construction_elliptic.ipynb\n",
      "   3. Run 03_gnn_baseline_training.ipynb\n",
      "\n",
      "üí° Tip: Open a terminal and run 'nvidia-smi -l 2' to monitor GPU usage\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checks = [\n",
    "    (\"PyTorch installed\", True),\n",
    "    (\"PyTorch Geometric installed\", True),\n",
    "    (\"CUDA available\", torch.cuda.is_available()),\n",
    "]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    checks.extend([\n",
    "        (\"GPU detected\", torch.cuda.device_count() > 0),\n",
    "        (\"GPU computation works\", True),\n",
    "        (\"PyG GPU support works\", True),\n",
    "    ])\n",
    "\n",
    "print(\"\\n‚úì Completed checks:\")\n",
    "for check, status in checks:\n",
    "    symbol = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"  {symbol} {check}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"\\n‚úÖ You're ready to train on GPU!\")\n",
    "    print(\"\\nüìù Next steps:\")\n",
    "    print(\"   1. Run 01_data_exploration.ipynb\")\n",
    "    print(\"   2. Run 02_graph_construction_elliptic.ipynb\")\n",
    "    print(\"   3. Run 03_gnn_baseline_training.ipynb\")\n",
    "    print(\"\\nüí° Tip: Open a terminal and run 'nvidia-smi -l 2' to monitor GPU usage\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  GPU not available\")\n",
    "    print(\"\\nüìù Options:\")\n",
    "    print(\"   1. Train on CPU (slower but works)\")\n",
    "    print(\"   2. Use Google Colab for free GPU\")\n",
    "    print(\"   3. Install CUDA-enabled PyTorch\")\n",
    "    print(\"\\nTo enable GPU:\")\n",
    "    print(\"  conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb54d1-8dfa-4c73-b7f6-f4c6bf81897e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
